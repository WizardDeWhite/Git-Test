https://www.kernel.org/doc/gorman/html/understand/understand009.html


1. __get_free_pages()
================================================================================
unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
{
	struct page *page;

	/*
	 * __get_free_pages() returns a 32-bit address, which cannot represent
	 * a highmem page
	 */
	VM_BUG_ON((gfp_mask & __GFP_HIGHMEM) != 0);

	page = alloc_pages(gfp_mask, order);
	if (!page)
		return 0;
	return (unsigned long) page_address(page);
}

1.1 alloc_pages(), when CONFIG_NUMA is defined
================================================================================
static inline struct page *
alloc_pages(gfp_t gfp_mask, unsigned int order)
{
	return alloc_pages_current(gfp_mask, order);
}

1.1.1 alloc_pages_current()
================================================================================
struct page *alloc_pages_current(gfp_t gfp, unsigned order)
{
	struct mempolicy *pol = &default_policy;
	struct page *page;
	unsigned int cpuset_mems_cookie;

	if (!in_interrupt() && !(gfp & __GFP_THISNODE))
		pol = get_task_policy(current);

retry_cpuset:
	cpuset_mems_cookie = read_mems_allowed_begin();

	/*
	 * No reference counting needed for current->mempolicy
	 * nor system default_policy
	 */
	if (pol->mode == MPOL_INTERLEAVE)
		page = alloc_page_interleave(gfp, order, interleave_nodes(pol));
	else
		page = __alloc_pages_nodemask(gfp, order,
				policy_zonelist(gfp, pol, numa_node_id()),
				policy_nodemask(gfp, pol));

	if (unlikely(!page && read_mems_allowed_retry(cpuset_mems_cookie)))
		goto retry_cpuset;

	return page;
}

1.1.1.1 __alloc_pages_nodemask()
================================================================================

1.1.1.2 policy_zonelist()
================================================================================

1.1.1.3 policy_nodemask()
================================================================================

1.2 page_address()
================================================================================
static inline void *page_address(const struct page *page)
{
	return page_to_virt(page);
}

2. __alloc_pages_nodemask()
================================================================================
struct page *
__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
			struct zonelist *zonelist, nodemask_t *nodemask)
{
	struct page *page;
	unsigned int alloc_flags = ALLOC_WMARK_LOW;
	gfp_t alloc_mask = gfp_mask; /* The gfp_t that was actually used for allocation */
	struct alloc_context ac = {
		.high_zoneidx = gfp_zone(gfp_mask),
		.zonelist = zonelist,
		.nodemask = nodemask,
		.migratetype = gfpflags_to_migratetype(gfp_mask),
	};

	if (cpusets_enabled()) {
		alloc_mask |= __GFP_HARDWALL;
		alloc_flags |= ALLOC_CPUSET;
		if (!ac.nodemask)
			ac.nodemask = &cpuset_current_mems_allowed;
	}

	gfp_mask &= gfp_allowed_mask;

	lockdep_trace_alloc(gfp_mask);

	might_sleep_if(gfp_mask & __GFP_DIRECT_RECLAIM);

	if (should_fail_alloc_page(gfp_mask, order))
		return NULL;

	/*
	 * Check the zones suitable for the gfp_mask contain at least one
	 * valid zone. It's possible to have an empty zonelist as a result
	 * of __GFP_THISNODE and a memoryless node
	 */
	if (unlikely(!zonelist->_zonerefs->zone))
		return NULL;

	if (IS_ENABLED(CONFIG_CMA) && ac.migratetype == MIGRATE_MOVABLE)
		alloc_flags |= ALLOC_CMA;

	/* Dirty zone balancing only done in the fast path */
	ac.spread_dirty_pages = (gfp_mask & __GFP_WRITE);

	/*
	 * The preferred zone is used for statistics but crucially it is
	 * also used as the starting point for the zonelist iterator. It
	 * may get reset for allocations that ignore memory policies.
	 */
	ac.preferred_zoneref = first_zones_zonelist(ac.zonelist,
					ac.high_zoneidx, ac.nodemask);
	if (!ac.preferred_zoneref->zone) {
		page = NULL;
		/*
		 * This might be due to race with cpuset_current_mems_allowed
		 * update, so make sure we retry with original nodemask in the
		 * slow path.
		 */
		goto no_zone;
	}

	/* First allocation attempt */
	page = get_page_from_freelist(alloc_mask, order, alloc_flags, &ac);
	if (likely(page))
		goto out;

no_zone:
	/*
	 * Runtime PM, block IO and its error handling path can deadlock
	 * because I/O on the device might not complete.
	 */
	alloc_mask = memalloc_noio_flags(gfp_mask);
	ac.spread_dirty_pages = false;

	/*
	 * Restore the original nodemask if it was potentially replaced with
	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
	 */
	if (unlikely(ac.nodemask != nodemask))
		ac.nodemask = nodemask;

	page = __alloc_pages_slowpath(alloc_mask, order, &ac);

out:
	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
	    unlikely(memcg_kmem_charge(page, gfp_mask, order) != 0)) {
		__free_pages(page, order);
		page = NULL;
	}

	if (kmemcheck_enabled && page)
		kmemcheck_pagealloc_alloc(page, order, gfp_mask);

	trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);

	return page;
}

2.1 get_page_from_freelist()
================================================================================

2.2 __alloc_pages_slowpath()
================================================================================

3. buffered_rmqueue()
================================================================================

3.1 rmqueue_bulk()
================================================================================

3.2 __rmqueue_smallest()
================================================================================

3.2.1 rmv_page_order(), remove a page
================================================================================

3.2.2 expand(), merge back if page is bigger then expect
================================================================================

3.3 __rmqueue()
================================================================================

3.4 __mod_zone_freepage_state()
================================================================================
static inline void __mod_zone_freepage_state(struct zone *zone, int nr_pages,
					     int migratetype)
{
	__mod_zone_page_state(zone, NR_FREE_PAGES, nr_pages);
	if (is_migrate_cma(migratetype))
		__mod_zone_page_state(zone, NR_FREE_CMA_PAGES, nr_pages);
}

3.5 zone_statistics()
================================================================================
void zone_statistics(struct zone *preferred_zone, struct zone *z, gfp_t flags)
{
	if (z->zone_pgdat == preferred_zone->zone_pgdat) {
		__inc_zone_state(z, NUMA_HIT);
	} else {
		__inc_zone_state(z, NUMA_MISS);
		__inc_zone_state(preferred_zone, NUMA_FOREIGN);
	}
	if (z->node == ((flags & __GFP_OTHER_NODE) ?
			preferred_zone->node : numa_node_id()))
		__inc_zone_state(z, NUMA_LOCAL);
	else
		__inc_zone_state(z, NUMA_OTHER);
}

4. free_all_bootmem(), release free pages to the buddy allocator for 1st time
================================================================================
; called by start_kernel->mm_init->mem_init

unsigned long __init free_all_bootmem(void)
{
	unsigned long pages;

	reset_all_zones_managed_pages();

	/*
	 * We need to use NUMA_NO_NODE instead of NODE_DATA(0)->node_id
	 *  because in some case like Node0 doesn't have RAM installed
	 *  low ram will be on Node1
	 */
	pages = free_low_memory_core_early();
	totalram_pages += pages;

	return pages;
}

4.1 reset_all_zones_managed_pages() 
================================================================================
void __init reset_all_zones_managed_pages(void)
{
	struct pglist_data *pgdat;

	if (reset_managed_pages_done)
		return;

	for_each_online_pgdat(pgdat)
		reset_node_managed_pages(pgdat);

	reset_managed_pages_done = 1;
}

4.1.1 reset_node_managed_pages() 
================================================================================
void reset_node_managed_pages(pg_data_t *pgdat)
{
	struct zone *z;

	for (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)
		z->managed_pages = 0;
}

4.2 free_low_memory_core_early()
================================================================================
static unsigned long __init free_low_memory_core_early(void)
{
	unsigned long count = 0;
	phys_addr_t start, end;
	u64 i;

	memblock_clear_hotplug(0, -1);

	for_each_free_mem_range(i, NUMA_NO_NODE, &start, &end, NULL)
		count += __free_memory_core(start, end);

#ifdef CONFIG_ARCH_DISCARD_MEMBLOCK
	{
		phys_addr_t size;

		/* Free memblock.reserved array if it was allocated */
		size = get_allocated_memblock_reserved_regions_info(&start);
		if (size)
			count += __free_memory_core(start, start + size);

		/* Free memblock.memory array if it was allocated */
		size = get_allocated_memblock_memory_regions_info(&start);
		if (size)
			count += __free_memory_core(start, start + size);
	}
#endif

	return count;
}

4.2.1 for_each_free_mem_range()
================================================================================
#define for_each_free_mem_range(i, nid, p_start, p_end, p_nid)		\
	for_each_mem_range(i, &memblock.memory, &memblock.reserved,	\
			   nid, p_start, p_end, p_nid)


#define for_each_mem_range(i, type_a, type_b, nid,			\
			   p_start, p_end, p_nid)			\
	for (i = 0, __next_mem_range(&i, nid, type_a, type_b,		\
				     p_start, p_end, p_nid);		\
	     i != (u64)ULLONG_MAX;					\
	     __next_mem_range(&i, nid, type_a, type_b,			\
			      p_start, p_end, p_nid))

4.2.1.1 __next_mem_range()
================================================================================
void __init_memblock __next_mem_range(u64 *idx, int nid,
				      struct memblock_type *type_a,
				      struct memblock_type *type_b,
				      phys_addr_t *out_start,
				      phys_addr_t *out_end, int *out_nid)
{
	int idx_a = *idx & 0xffffffff;
	int idx_b = *idx >> 32;

	if (WARN_ONCE(nid == MAX_NUMNODES,
	"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
		nid = NUMA_NO_NODE;

	for (; idx_a < type_a->cnt; idx_a++) {
		struct memblock_region *m = &type_a->regions[idx_a];

		phys_addr_t m_start = m->base;
		phys_addr_t m_end = m->base + m->size;
		int	    m_nid = memblock_get_region_node(m);

		/* only memory regions are associated with nodes, check it */
		if (nid != NUMA_NO_NODE && nid != m_nid)
			continue;

		/* skip hotpluggable memory regions if needed */
		if (movable_node_is_enabled() && memblock_is_hotpluggable(m))
			continue;

		if (!type_b) {
			if (out_start)
				*out_start = m_start;
			if (out_end)
				*out_end = m_end;
			if (out_nid)
				*out_nid = m_nid;
			idx_a++;
			*idx = (u32)idx_a | (u64)idx_b << 32;
			return;
		}

		/* scan areas before each reservation */
		for (; idx_b < type_b->cnt + 1; idx_b++) {
			struct memblock_region *r;
			phys_addr_t r_start;
			phys_addr_t r_end;

			r = &type_b->regions[idx_b];
			r_start = idx_b ? r[-1].base + r[-1].size : 0;
			r_end = idx_b < type_b->cnt ?
				r->base : ULLONG_MAX;

			/*
			 * if idx_b advanced past idx_a,
			 * break out to advance idx_a
			 */
			if (r_start >= m_end)
				break;
			/* if the two regions intersect, we're done */
			if (m_start < r_end) {
				if (out_start)
					*out_start =
						max(m_start, r_start);
				if (out_end)
					*out_end = min(m_end, r_end);
				if (out_nid)
					*out_nid = m_nid;
				/*
				 * The region which ends first is
				 * advanced for the next iteration.
				 */
				if (m_end <= r_end)
					idx_a++;
				else
					idx_b++;
				*idx = (u32)idx_a | (u64)idx_b << 32;
				return;
			}
		}
	}

	/* signal end of iteration */
	*idx = ULLONG_MAX;
}

4.2.2 __free_memory_core(), free page with max order
================================================================================
static unsigned long __init __free_memory_core(phys_addr_t start,
				 phys_addr_t end)
{
	unsigned long start_pfn = PFN_UP(start);
	unsigned long end_pfn = min_t(unsigned long,
				      PFN_DOWN(end), max_low_pfn);

	if (start_pfn > end_pfn)
		return 0;

	__free_pages_memory(start_pfn, end_pfn);

	return end_pfn - start_pfn;
}

static void __init __free_pages_memory(unsigned long start, unsigned long end)
{
	int order;

	while (start < end) {
		order = min(MAX_ORDER - 1UL, __ffs(start));

		while (start + (1UL << order) > end)
			order--;

		__free_pages_bootmem(pfn_to_page(start), order);

		start += (1UL << order);
	}
}

4.2.2.1 pfn_to_page(), the flat version
================================================================================
/* memmap is virtually contiguous.  */
#define __pfn_to_page(pfn)	(vmemmap + (pfn))
#define __page_to_pfn(page)	(unsigned long)((page) - vmemmap)

4.2.2.2 __free_pages_bootmem()
================================================================================
void __init __free_pages_bootmem(struct page *page, unsigned int order)
{
	unsigned int nr_pages = 1 << order;
	struct page *p = page;
	unsigned int loop;

	prefetchw(p);
	for (loop = 0; loop < (nr_pages - 1); loop++, p++) {
		prefetchw(p + 1);
		__ClearPageReserved(p);
		set_page_count(p, 0);
	}
	__ClearPageReserved(p);
	set_page_count(p, 0);

	page_zone(page)->managed_pages += nr_pages;
	set_page_refcounted(page);
	__free_pages(page, order);
}

4.3 totalram_pages += pages
================================================================================

5 __free_pages()
================================================================================
void __free_pages(struct page *page, unsigned int order)
{
	if (put_page_testzero(page)) {
		if (order == 0)
			free_hot_cold_page(page, false);
		else
			__free_pages_ok(page, order);
	}
}

5.1 free_hot_cold_page()
================================================================================
/*
 * Free a 0-order page
 * cold == true ? free a cold page : free a hot page
 */
void free_hot_cold_page(struct page *page, bool cold)
{
	struct zone *zone = page_zone(page);
	struct per_cpu_pages *pcp;
	unsigned long flags;
	unsigned long pfn = page_to_pfn(page);
	int migratetype;

	if (!free_pcp_prepare(page))
		return;

	migratetype = get_pfnblock_migratetype(page, pfn);
	set_pcppage_migratetype(page, migratetype);
	local_irq_save(flags);
	__count_vm_event(PGFREE);

	/*
	 * We only track unmovable, reclaimable and movable on pcp lists.
	 * Free ISOLATE pages back to the allocator because they are being
	 * offlined but treat RESERVE as movable pages so we can get those
	 * areas back if necessary. Otherwise, we may have to free
	 * excessively into the page allocator
	 */
	if (migratetype >= MIGRATE_PCPTYPES) {
		if (unlikely(is_migrate_isolate(migratetype))) {
			free_one_page(zone, page, pfn, 0, migratetype);
			goto out;
		}
		migratetype = MIGRATE_MOVABLE;
	}

	pcp = &this_cpu_ptr(zone->pageset)->pcp;
	if (!cold)
		list_add(&page->lru, &pcp->lists[migratetype]);
	else
		list_add_tail(&page->lru, &pcp->lists[migratetype]);
	pcp->count++;
	if (pcp->count >= pcp->high) {
		unsigned long batch = READ_ONCE(pcp->batch);
		free_pcppages_bulk(zone, batch, pcp);
		pcp->count -= batch;
	}

out:
	local_irq_restore(flags);
}

5.2 __free_pages_ok()
================================================================================

6 __free_one_page(), freeing function for a buddy system allocator
================================================================================
static inline void __free_one_page(struct page *page,
		unsigned long pfn,
		struct zone *zone, unsigned int order,
		int migratetype)
{
	unsigned long page_idx;
	unsigned long combined_idx;
	unsigned long uninitialized_var(buddy_idx);
	struct page *buddy;
	int max_order = MAX_ORDER;

	VM_BUG_ON(!zone_is_initialized(zone));
	VM_BUG_ON_PAGE(page->flags & PAGE_FLAGS_CHECK_AT_PREP, page);

	VM_BUG_ON(migratetype == -1);
	if (is_migrate_isolate(migratetype)) {
		/*
		 * We restrict max order of merging to prevent merge
		 * between freepages on isolate pageblock and normal
		 * pageblock. Without this, pageblock isolation
		 * could cause incorrect freepage accounting.
		 */
		max_order = min(MAX_ORDER, pageblock_order + 1);
	} else {
		__mod_zone_freepage_state(zone, 1 << order, migratetype);
	}

	page_idx = pfn & ((1 << max_order) - 1);

	VM_BUG_ON_PAGE(page_idx & ((1 << order) - 1), page);
	VM_BUG_ON_PAGE(bad_range(zone, page), page);

	while (order < max_order - 1) {
		buddy_idx = __find_buddy_index(page_idx, order);
		buddy = page + (buddy_idx - page_idx);
		if (!page_is_buddy(page, buddy, order))
			break;
		/*
		 * Our buddy is free or it is CONFIG_DEBUG_PAGEALLOC guard page,
		 * merge with it and move up one order.
		 */
		if (page_is_guard(buddy)) {
			clear_page_guard(zone, buddy, order, migratetype);
		} else {
			list_del(&buddy->lru);
			zone->free_area[order].nr_free--;
			rmv_page_order(buddy);
		}
		combined_idx = buddy_idx & page_idx;
		page = page + (combined_idx - page_idx);
		page_idx = combined_idx;
		order++;
	}
	set_page_order(page, order);

	/*
	 * If this is not the largest possible page, check if the buddy
	 * of the next-highest order is free. If it is, it's possible
	 * that pages are being freed that will coalesce soon. In case,
	 * that is happening, add the free page to the tail of the list
	 * so it's less likely to be used soon and more likely to be merged
	 * as a higher order page
	 */
	if ((order < MAX_ORDER-2) && pfn_valid_within(page_to_pfn(buddy))) {
		struct page *higher_page, *higher_buddy;
		combined_idx = buddy_idx & page_idx;
		higher_page = page + (combined_idx - page_idx);
		buddy_idx = __find_buddy_index(combined_idx, order + 1);
		higher_buddy = higher_page + (buddy_idx - combined_idx);
		if (page_is_buddy(higher_page, higher_buddy, order + 1)) {
			list_add_tail(&page->lru,
				&zone->free_area[order].free_list[migratetype]);
			goto out;
		}
	}

	list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
out:
	zone->free_area[order].nr_free++;
}

6.1 __find_buddy_index()
================================================================================
; The comment is interesting
;
; When locating the "buddy", it switch on/off the (1<<O) bit in the index
; When locating the "parent", it just clear the (1<<O)) bit in the index
;
; Which leads to "parent" = "buddy" & "`buddy", just like the code
;            combined_idx = buddy_idx & page_idx;

/*
 * Locate the struct page for both the matching buddy in our
 * pair (buddy1) and the combined O(n+1) page they form (page).
 *
 * 1) Any buddy B1 will have an order O twin B2 which satisfies
 * the following equation:
 *     B2 = B1 ^ (1 << O)
 * For example, if the starting buddy (buddy2) is #8 its order
 * 1 buddy is #10:
 *     B2 = 8 ^ (1 << 1) = 8 ^ 2 = 10
 *
 * 2) Any buddy B will have an order O+1 parent P which
 * satisfies the following equation:
 *     P = B & ~(1 << O)
 *
 * Assumption: *_mem_map is contiguous at least up to MAX_ORDER
 */
static inline unsigned long
__find_buddy_index(unsigned long page_idx, unsigned int order)
{
	return page_idx ^ (1 << order);
}

6.2 page_is_buddy()
================================================================================
static inline int page_is_buddy(struct page *page, struct page *buddy,
							unsigned int order)
{
	if (!pfn_valid_within(page_to_pfn(buddy)))
		return 0;

	if (page_is_guard(buddy) && page_order(buddy) == order) {
		if (page_zone_id(page) != page_zone_id(buddy))
			return 0;

		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);

		return 1;
	}

	if (PageBuddy(buddy) && page_order(buddy) == order) {
		/*
		 * zone check is done late to avoid uselessly
		 * calculating zone/node ids for pages that could
		 * never merge.
		 */
		if (page_zone_id(page) != page_zone_id(buddy))
			return 0;

		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);

		return 1;
	}
	return 0;
}

6.3 clear_page_guard()
================================================================================

6.4 pfn_valid_within()
================================================================================

7. free_area_init()
================================================================================
7.1 arch_zone_lowest_possible_pfn / arch_zone_highest_possible_pfn
================================================================================
7.2 zone_movable_pfn
================================================================================
7.3 subsection_map_init(), set mem_section->usage->subsection_map
================================================================================
7.4 setup_nr_node_ids()
================================================================================
7.5 free_area_init_node(nid), iterate on each node
================================================================================
7.5.1 get_pfn_range_for_nid(nid, &start_pfn, &end_pfn)
================================================================================
7.5.2 calculate_node_totalpages(pgdat, start_pfn, end_pfn)
================================================================================
initialize
zone->spanned_pages/present_pages
pgdat->node_spanned_pages/node_present_pages
7.5.2.1 spanned = zone_spanned_pages_in_node()
================================================================================
7.5.2.2 absent = zone_absent_pages_in_node()
================================================================================
7.5.3 free_area_init_core(pgdat)
================================================================================
7.5.3.1 pgdat_init_internals()
================================================================================
7.5.3.2 calc_memmap_size()
================================================================================
7.5.3.3 zone_init_internals(), setup zone->managed_pages, but will be reset later
================================================================================
7.5.3.4 setup_usemap(zone), used for non-sparse
================================================================================
7.5.3.5 init_currently_empty_zone(), init free_lists
================================================================================
7.5.4 lru_gen_init_pgdat()
================================================================================
7.6 node_set_state(nid, N_MEMORY)
================================================================================
7.7 memmap_init()
================================================================================
7.7.1 memmap_init_zone_range(zone, start_pfn, end_pfn, hole_pfn)
================================================================================
7.7.1.1 memmap_init_range(size, nid, zone, start_pfn, zone_end_pfn, context, altmap, migratetype)
================================================================================
7.7.1.1.1 __init_single_page(page, pfn, zone, nid)
================================================================================
7.7.1.1.2 set_pageblock_migratetype(page, migratetype)
================================================================================
7.7.2 init_unavailable_range(hole_pfn, end_pfn, zone_id, nid)
================================================================================

10. init_per_zone_wmark_min(), called from do_pre_smp_initcalls()
================================================================================

10.1 nr_free_buffer_pages()
================================================================================
10.2 setup_per_zone_wmarks() -> __setup_per_zone_wmarks()
================================================================================
10.2.1 lowmem_pages += zone_managed_pages(zone);
================================================================================
10.2.2 tmp = (u64)pages_min * zone_managed_pages(zone);
================================================================================
10.2.3 do_div(tmp, lowmem_pages);
================================================================================
10.2.4 zone->_watermark[WMARK_MIN] = tmp;
================================================================================
10.2.5 zone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;
================================================================================
10.2.6 zone->_watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;
================================================================================
10.3 refresh_zone_stat_thresholds()
================================================================================
10.4 setup_per_zone_lowmem_reserve()
================================================================================
10.5 setup_min_unmapped_ratio()
================================================================================
10.6 setup_min_slab_ratio()
================================================================================

11. alloc_contig_range(start, end, migratetype, gfp_mask)
================================================================================
11.0 cc = {.mode = MIGRATE_SYNC, alloc_contig = true}
================================================================================
11.1 start_isolate_page_range(pfn_max_align_down(start), pfn_max_align_up(end), migratetype, 0)
================================================================================
11.1.1 set_migratetype_isolate(page, migratetype, flags)
================================================================================
11.1.1.1 unmovable = has_unmovable_pages(zone, page, migratetype, isol_flags);
================================================================================
11.1.1.2 set_pageblock_migratetype(page, MIGRATE_ISOLATE)
================================================================================
11.1.1.3 move_freepages_block(zone, page, MIGRATE_ISOLATE, NULL)
================================================================================
11.1.1.4 drain_all_pages(zone)
================================================================================
11.2 __alloc_contig_migrate_range(&cc, start, end)
================================================================================
11.2.1 mtc = {}
================================================================================
11.2.2 migrate_prep()
================================================================================
11.2.3 pfn = isolate_migratepages_range(cc, pfn, end)
================================================================================
11.2.3.1 isolate_migratepages_block(cc, pfn, , ISOLATE_UNEVICTABLE)
================================================================================
11.2.4 nr_reclaimed = reclaim_clean_pages_from_list(cc->zone, cc->migratepages)
================================================================================
11.2.5 cc->nr_migratepages -= nr_reclaimed
================================================================================
11.2.6 migrate_pages(cc->migratepages, )
================================================================================
11.3 lru_add_drain_all()
================================================================================
11.4 test_pages_isolated(outer_start, end, 0)
================================================================================
11.5 isolate_freepages_range(&cc, outer_start, end)
================================================================================
11.6 undo_isolate_page_range(pfn_max_align_down(start), pfn_max_align_up(end), migratetype)
================================================================================

0. data structure
================================================================================
http://wiki.osdev.org/Detecting_Memory_(x86)

0.1 arch_zone_lowest/highest_possible_pfn[]
================================================================================
static unsigned long arch_zone_lowest_possible_pfn[MAX_NR_ZONES];
static unsigned long arch_zone_highest_possible_pfn[MAX_NR_ZONES];

; dmesg: "Zone ranges:"
; an example on x86_64

   Memory

                 16M                   4G                            6G
   [   ZONE_DMA   |      ZONE_DMA32     |            ZONE_NORMAL      ]
   ^              ^                     ^                             ^
   |              |                     |                             |
lowest_pfn[0]  highest_pfn[0]       highest_pfn[1]               highest_pfn[2]
                lowest_pfn[1]        lowest_pfn[2]


0.2 node
================================================================================

   Memory

                                3G                                   6G
   [           Node0             |                Node1               ]

0.3 node/zone
================================================================================

   Memory

                 16M                   4G                            6G
   [   ZONE_DMA   |      ZONE_DMA32     |            ZONE_NORMAL      ]
                                3G
   ^                             ^                                    ^
   |<---      Node0          --->|<---          Node1             --->|

0.4.1 node/zone ranges
================================================================================

   node_data[0]                   
   +-----------------------------+
   |node_id                <-----|-----+
   |   (int)                     |     |
   |node_start_pfn               |     |
   |node_spanned_pages           |     |
   |node_present_pages           |     |
   |   (unsigned long)           |     |
   |                             |     |
   +-----------------------------+     |
   |node_zones[MAX_NR_ZONES]     |     |
   |   (struct zone)             |     |
   |   +-------------------------+     |
   |   |zone_pgdat         ------|-----+
   |   |                         |
   |   |zone_start_pfn           |
   |   |spanned_pages            |
   |   |present_pages            |
   |   |present_early_pages      |
   |   |  (unsigned long)        |
   |   |                         |
   +---+-------------------------+


0.4 representation
================================================================================
; node_data[] allocated in alloc_node_data()

   node_data[0]                                                node_data[1]
   +-----------------------------+                             +-----------------------------+        
   |node_id                <---+ |                             |node_id                <---+ |        
   |   (int)                   | |                             |   (int)                   | |        
   +-----------------------------+                             +-----------------------------+    
   |node_zones[MAX_NR_ZONES]   | |    [ZONE_DMA]               |node_zones[MAX_NR_ZONES]   | |    [ZONE_DMA]       
   |   (struct zone)           | |    +---------------+        |   (struct zone)           | |    +---------------+
   |   +-------------------------+    |0              |        |   +-------------------------+    |empty          |
   |   |                       | |    |16M            |        |   |                       | |    |               |
   |   |zone_pgdat         ----+ |    +---------------+        |   |zone_pgdat         ----+ |    +---------------+
   |   |                         |                             |   |                         |        
   |   |                         |    [ZONE_DMA32]             |   |                         |    [ZONE_DMA32]        
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |    |16M            |        |   |                         |    |3G             |   
   |   |                         |    |3G             |        |   |                         |    |4G             |   
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |                             |   |                         |        
   |   |                         |    [ZONE_NORMAL]            |   |                         |    [ZONE_NORMAL]       
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |    |empty          |        |   |                         |    |4G             |   
   |   |                         |    |               |        |   |                         |    |6G             |   
   +---+-------------------------+    +---------------+        +---+-------------------------+    +---------------+


0.5 alloc_pages() called by
================================================================================

    alloc_pages()

	__get_free_pages()
	alloc_pages_vma()
	alloc_hugepage_vma()
	alloc_slab_page()

0.6 per_cpu_pages
================================================================================

0.7 compound page
================================================================================

prep_compound_page
    __SetPageHead(page)
    set_compound_head(p, page)

free_tail_pages_check
    clear_compound_head()


NULL_COMPOUND_DTOR
COMPOUND_PAGE_DTOR
HUGETLB_PAGE_DTOR
TRANSHUGE_PAGE_DTOR

    page                  page                  page                  page
    +----------------+    +----------------+    +----------------+    +----------------+
    |PG_HEAD         |    |                |    |                |    |                |
    |compound_order  |    |                |    |                |    |                |
    |compound_dtor   |    |                |    |                |    |                |
    |                |    |                |    |                |    |                |
    |                |    |compound_head + |    |compound_head + |    |compound_head + |
    +----------------+    +--------------|-+    +--------------|-+    +--------------|-+
    ^                                    |                     |                     |
    |                                    |                     |                     |
    +------------------------------------+---------------------+---------------------+

0.8 zone watermark
================================================================================

  lowmem_pages = total lowmem
  zone->_watermark[min] = (current zone lowmem * pages_min) / total lowmem

  =>  sum(zone->_watermark[WMARK_MIN]) = pages_min


  tmp = (zone_managed_pages(z) * watermark_scale_factor) / 10000

  zone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;
  zone->_watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;

  The bigger watermark_scale_factor, the bigger gap between MIN, LOW and HIGH.

0.9 lowmem reserve
================================================================================

setup_per_zone_lowmem_reserve()

abbreviation:
mp[i]: managed pages of zone[i]
ra[i]: sysctl_lowmem_reserve_ratio[i]
lr[i]: lowmem_reserve[i]


        +-------------+-------------+------------+------------+
  lr[3] |mp[3] + mp[2]|mp[3] + mp[2]|mp[3]       |     0      |
        |+ mp[1] /    |   /         |   /        |            |
        |ra[0]        |ra[1]        |ra[2]       |            |
        +-------------+-------------+------------+------------+
  lr[2] |mp[2] + mp[1]|mp[2]        |     0      |   N/A      |
        |   /         |   /         |            |            |
        |ra[0]        |ra[1]        |            |            |
        +-------------+-------------+------------+------------+
  lr[1] |mp[1]        |     0       |   N/A      |   N/A      |
        |   /         |             |            |            |
        |ra[0]        |             |            |            |
        +-------------+-------------+------------+------------+
  lr[0] |     0       |   N/A       |   N/A      |   N/A      |
        |             |             |            |            |
        |             |             |            |            |
        +-------------+-------------+------------+------------+
        Zone[0]       Zone[1]      Zone[2]      Zone[3]

The zone[i].lowmem_reserve[j] means if the allocation for zone[j] fall back to
zone[i], the allocation would succeed if we have more than lowmem_reserve[j]
memory.

The is used to check the watermark in __zone_watermark_ok()

```
	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
		return false;
```

If free_pages is less then this, watermark will alarm.

0.5 vm_stat
================================================================================
__mod_zone_page_state()
__mod_node_page_state()

atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;
atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS] __cacheline_aligned_in_smp;
atomic_long_t vm_numa_event[NR_VM_NUMA_EVENT_ITEMS] __cacheline_aligned_in_smp;

   node_data[MAX_NUMNODES] (struct pglist_data / pg_data_t)
   +-----------------------------------+
   |node_id                            |
   |   (int)                           |
   +-----------------------------------+
   |node_zones[MAX_NR_ZONES]           |
   |   (struct zone)                   |
   |   +-------------------------------+
   |   |per_cpu_zonestats              |
   |   |   +---------------------------+
   |   |   |stat_threshold             |
   |   |   |vm_stat_diff[]             |
   |   |   +---------------------------+
   |   |                               |
   |   |vm_stat[NR_VM_ZONE_STAT_ITEMS] |
   |   |vm_numa_event[]                |
   |   |  (atomic_long_t)              |
   +---+-------------------------------+
   |per_cpu_nodestats                  |
   |   +-------------------------------+
   |   |stat_threshold                 |
   |   |vm_node_stat_diff[]            |
   |   +-------------------------------+
   |                                   |
   |vm_stat[NR_VM_NODE_STAT_ITEMS]     |
   |   (atomic_long_t)                 |
   |                                   |
   +-----------------------------------+

0.2 nodemask_t node_stats[NR_NODE_STATES]
================================================================================

typedef struct { DECLARE_BITMAP(bits, MAX_NUMNODES); } nodemask_t;

/*
 * Bitmasks that are kept for all the nodes.
 */
enum node_states {
	N_POSSIBLE,		/* The node could become online at some point */
	N_ONLINE,		/* The node is online */
	N_NORMAL_MEMORY,	/* The node has regular memory */
#ifdef CONFIG_HIGHMEM
	N_HIGH_MEMORY,		/* The node has regular or high memory */
#else
	N_HIGH_MEMORY = N_NORMAL_MEMORY,
#endif
#ifdef CONFIG_MOVABLE_NODE
	N_MEMORY,		/* The node has memory(regular, high, movable) */
#else
	N_MEMORY = N_HIGH_MEMORY,
#endif
	N_CPU,		/* The node has one or more cpus */
	NR_NODE_STATES
};

0.3 node_data[], each node has one
================================================================================
; node_data[MAX_NUMNODES] is an array with pointers to pg_data_t.
;
; It is allocated in numa_register_memblks(). Each node with proper memory
; would has one entry in node_data[].
;
; Then free_area_init_node() sets
; * node_start_pfn/node_present_pages
; * zone->start_pfn/spanned_pages
;
; The zonelist in each node_data is built by
; build_zonelists()/build_zonelist_cache().  node_data->node_zonelists[] will
; point to zones in all node_data.


MAX_NR_ZONES is defined in kernel/bounds.c DEFINE(MAX_NR_ZONES, __MAX_NR_ZONES);
MAX_ZONELISTS is defined to 1 or 2.
#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)

   node_data[MAX_NUMNODES] (struct pglist_data / pg_data_t)
   +-----------------------------+        
   |node_id                <---+ |        
   |   (int)                   | |        
   +-----------------------------+        
   |node_start_pfn             | |        
   |   (unsigned long)         | |        
   |node_present_pages         | |        
   |node_spanned_pages         | |        
   |   (unsigned long)         | |        
   |                           | |        
   +-----------------------------+        
   |nr_zones                   | |        
   |    (int)                  | |        
   |                           | |        
   +-----------------------------+    
   |node_zones[MAX_NR_ZONES]   | |    [ZONE_DMA]           [ZONE_DMA32]         [ZONE_NORMAL]
   |   (struct zone)           | |    +---------------+    +---------------+    +---------------+
   |   +-------------------------+    |               |    |               |    |               |
   |   |                       | |    |               |    |               |    |               |
   |   |zone_pgdat         ----+ |    +---------------+    +---------------+    +---------------+
   |   |   (struct pglist_data*) |
   |   +-------------------------+
   |   |zone_start_pfn           |
   |   |  (unsigned long)        |
   |   |spanned_pages            |
   |   |present_pages            |
   |   |managed_pages            |
   |   |  (unsigned long)        |
   |   |                         |
   +---+-------------------------+                                                                               
   |node_zonelists[MAX_ZONELISTS]|
   |   (struct zonelist)         |
   |   +-------------------------+
   |   |_zonerefs[]              | = MAX_NUMNODES * MAX_NR_ZONES + 1
   |   | (struct zoneref)        | Node 0:
   |   |  +----------------------+
   |   |  |zone                  |    [ZONE_NORMAL]        [ZONE_DMA32]         [ZONE_DMA]
   |   |  |   (struct zone*)     |    +---------------+    +---------------+    +---------------+
   |   |  |zone_idx              |    |               |    |               |    |               |
   |   |  |   (int)              |    |               |    |               |    |               |
   |   |  |                      |    +---------------+    +---------------+    +---------------+
   |   |  |                      |
   |   |  |                      | Node 1:
   |   |  |                      |
   |   |  |                      |    [ZONE_NORMAL]        [ZONE_DMA32]         [ZONE_DMA]
   |   |  |                      |    +---------------+    +---------------+    +---------------+
   |   |  |                      |    |               |    |               |    |               |
   |   |  |                      |    |               |    |               |    |               |
   |   |  |                      |    +---------------+    +---------------+    +---------------+
   |   |  |                      |
   |   |  |                      |
   +---+--+----------------------+
   |reclaim_nodes                |
   |    (nodemask_t)             |
   +-----------------------------+
   |kswapd_wait                  |
   |    (wait_queue_head_t)      |
   +-----------------------------+
   |pfmemalloc_wait              |                                   
   |    (wait_queue_head_t)      |
   +-----------------------------+
   |lru_lock                     |
   |    (spinlock_t)             |
   +-----------------------------+
   |lruvec                       |
   |    (struct lruvec)          |
   |    +------------------------+
   |    |lists[NR_LRU_LISTS]     |   5 lru lists
   |    |reclaim_stat            |
   |    |inactive_age            |
   |    |refaults                |
   |    +------------------------+
   |                             |
   |                             |
   +-----------------------------+

0.3.1 node_data/zone, real example
================================================================================


   node_data[0]                                                node_data[1]
   +-----------------------------+                             +-----------------------------+        
   |node_id                <---+ |                             |node_id                <---+ |        
   |   (int)                   | |                             |   (int)                   | |        
   +-----------------------------+                             +-----------------------------+    
   |node_zones[MAX_NR_ZONES]   | |    [ZONE_DMA]               |node_zones[MAX_NR_ZONES]   | |    [ZONE_DMA]       
   |   (struct zone)           | |    +---------------+        |   (struct zone)           | |    +---------------+
   |   +-------------------------+    |0              |        |   +-------------------------+    |empty          |
   |   |                       | |    |16M            |        |   |                       | |    |               |
   |   |zone_pgdat         ----+ |    +---------------+        |   |zone_pgdat         ----+ |    +---------------+
   |   |                         |                             |   |                         |        
   |   |                         |    [ZONE_DMA32]             |   |                         |    [ZONE_DMA32]        
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |    |16M            |        |   |                         |    |3G             |   
   |   |                         |    |3G             |        |   |                         |    |4G             |   
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |                             |   |                         |        
   |   |                         |    [ZONE_NORMAL]            |   |                         |    [ZONE_NORMAL]       
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |    |empty          |        |   |                         |    |4G             |   
   |   |                         |    |               |        |   |                         |    |6G             |   
   +---+-------------------------+    +---------------+        +---+-------------------------+    +---------------+

0.3.2 node_data/zonelist
================================================================================

   node_data[]
   +-----------------------------+
   |node_zonelists[MAX_ZONELISTS]| = 2
   |   (struct zonelist)         |
   |   +-------------------------+
   |   |_zonerefs[]              | = MAX_NUMNODES * MAX_NR_ZONES + 1
   |   | (struct zoneref)        | Node 0:
   |   |  +----------------------+    [ZONE_NORMAL]        [ZONE_DMA32]         [ZONE_DMA]
   |   |  |zone                  |    +---------------+    +---------------+    +---------------+
   |   |  |   (struct zone*)     |    |               |    |               |    |               |
   |   |  |zone_idx              |    |               |    |               |    |               |
   |   |  |   (int)              |    +---------------+    +---------------+    +---------------+
   +---+--+----------------------+
                                   Node 1:

                                      [ZONE_NORMAL]        [ZONE_DMA32]         [ZONE_DMA]
                                      +---------------+    +---------------+    +---------------+
                                      |               |    |               |    |               |
                                      |               |    |               |    |               |
                                      +---------------+    +---------------+    +---------------+

0.4 zone
================================================================================
; zone->managed_pages
;
; calculated in free_area_init_core()
; cleared at free_all_bootmem()
; increased at __free_pages_boot_core()


      struct zone
      +------------------------------+
      |watermark[NR_WMARK]           |
      |   (unsigned long)            |
      +------------------------------+
      |zone_start_pfn                |
      |   (unsigned long)            |
      |spanned_pages                 |
      |present_pages                 |
      |managed_pages                 |
      |   (unsigned long)            |
      +------------------------------+
      |percpu_drift_mark             |
      |   (unsigned long)            |
      +------------------------------+
      |pageset                       |
      |   (struct per_cpu_pageset *) |
      |   +--------------------------+
      |   |pcp                       |
      |   |  (struct per_cpu_pages)  |
      |   |  +-----------------------+
      |   |  |count                  |
      |   |  |high                   |
      |   |  |batch                  |
      |   |  |                       |
      |   |  |lists[MIGRATE_PCPTYPES]|
      +---+--+-----------------------+
      |lowmem_reserve[MAX_NR_ZONES]  |
      |   (unsigned long)            |
      +------------------------------+
      |dirty_balance_reserve         |
      |   (unsigned long)            |
      +------------------------------+
      |vm_stat[NR_VM_ZONE_STAT_ITEMS]|
      |   (atomic_long_t)            |
      +------------------------------+
      |inactive_ratio                |
      |   (unsigned int)             |
      +------------------------------+      The buddy system
      |free_area[MAX_ORDER]  0...11  |
      |   (struct free_area)         | 
      |   +--------------------------+
      |   |nr_free                   |  number of available pages
      |   |(unsigned long)           |  in this free_area[] list
      |   |                          |
      |   +--------------------------+
      |   |                          |           free_area[0]
      |   |free_list[MIGRATE_TYPES]  |  Order0   +-----------------------+
      |   |(struct list_head)        |  Pages    |free_list              |
      |   |                          |           |  (struct list_head)   |
      |   |                          |           +-----------------------+
      |   |                          |
      |   |                          |           free_area[1]
      |   |                          |  Order1   +-----------------------+
      |   |                          |  Pages    |free_list              |
      |   |                          |           |  (struct list_head)   |
      |   |                          |           +-----------------------+
      |   |                          |
      |   |                          |
      |   |                          |
      |   |                          |           free_area[10]
      |   |                          |  Order10  +-----------------------+
      |   |                          |  Pages    |free_list              |
      |   |                          |           |  (struct list_head)   |
      |   |                          |           +-----------------------+
      |   |                          |
      |   |                          |
      |   |                          |           free_area[11]
      |   |                          |  Order11  +-----------------------+
      |   |                          |  Pages    |free_list              |
      |   |                          |           |  (struct list_head)   |
      |   |                          |           +-----------------------+
      |   |                          |
      +---+--------------------------+
      |                              |
      |                              |
      |                              |
      |                              |
      |                              |
      +------------------------------+
           

0.4.1 zone->pageset
================================================================================

per cpu variable, each zone maintains the status of each cpu

      struct zone
      +------------------------------------------------------------------------------------------------+
      |pageset                                                                                         |
      |   (struct per_cpu_pageset *)                                                                   |
      |   cpu0                          cpu1                                cpuN                       |
      |   +--------------------------+  +--------------------------+  ...   +--------------------------+
      |   |pcp                       |  |pcp                       |        |pcp                       |
      |   |  (struct per_cpu_pages)  |  |  (struct per_cpu_pages)  |        |  (struct per_cpu_pages)  |
      |   |  +-----------------------+  |  +-----------------------+        |  +-----------------------+
      |   |  |count                  |  |  |count                  |        |  |count                  |
      |   |  |high                   |  |  |high                   |        |  |high                   |
      |   |  |batch                  |  |  |batch                  |        |  |batch                  |
      |   |  |                       |  |  |                       |        |  |                       |
      |   |  |lists[MIGRATE_PCPTYPES]|  |  |lists[MIGRATE_PCPTYPES]|        |  |lists[MIGRATE_PCPTYPES]|
      +---+--+-----------------------+--+--+-----------------------+--------+--+-----------------------+

0.5 GFP_ZONE_TABLE lowest 4 bits gfp_t to zone index
================================================================================

The lowest 4 bits represent zone information in GFP.

#define ___GFP_DMA		0x01u
#define ___GFP_HIGHMEM		0x02u
#define ___GFP_DMA32		0x04u
#define ___GFP_MOVABLE		0x08u

#define __GFP_DMA	((__force gfp_t)___GFP_DMA)
#define __GFP_HIGHMEM	((__force gfp_t)___GFP_HIGHMEM)
#define __GFP_DMA32	((__force gfp_t)___GFP_DMA32)
#define __GFP_MOVABLE	((__force gfp_t)___GFP_MOVABLE)  /* ZONE_MOVABLE allowed */
#define GFP_ZONEMASK	(__GFP_DMA|__GFP_HIGHMEM|__GFP_DMA32|__GFP_MOVABLE)

A 4 bits could have 2^4 = 16 combinations.

Since there are 8 BAD combinations,

#define GFP_ZONE_BAD ( \
	1 << (___GFP_DMA | ___GFP_HIGHMEM)				      \
	| 1 << (___GFP_DMA | ___GFP_DMA32)				      \
	| 1 << (___GFP_DMA32 | ___GFP_HIGHMEM)				      \
	| 1 << (___GFP_DMA | ___GFP_DMA32 | ___GFP_HIGHMEM)		      \
	| 1 << (___GFP_MOVABLE | ___GFP_HIGHMEM | ___GFP_DMA)		      \
	| 1 << (___GFP_MOVABLE | ___GFP_DMA32 | ___GFP_DMA)		      \
	| 1 << (___GFP_MOVABLE | ___GFP_DMA32 | ___GFP_HIGHMEM)		      \
	| 1 << (___GFP_MOVABLE | ___GFP_DMA32 | ___GFP_DMA | ___GFP_HIGHMEM)  \
)

Totally there are 8 valid ones.

#define GFP_ZONE_TABLE ( \
	(ZONE_NORMAL << 0 * GFP_ZONES_SHIFT)				       \
	| (OPT_ZONE_DMA << ___GFP_DMA * GFP_ZONES_SHIFT)		       \
	| (OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * GFP_ZONES_SHIFT)	       \
	| (OPT_ZONE_DMA32 << ___GFP_DMA32 * GFP_ZONES_SHIFT)		       \
	| (ZONE_NORMAL << ___GFP_MOVABLE * GFP_ZONES_SHIFT)		       \
	| (OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * GFP_ZONES_SHIFT)    \
	| (ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * GFP_ZONES_SHIFT)\
	| (OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * GFP_ZONES_SHIFT)\

This macro encode zone information into GFP_ZONE_TABLE.

         GFP_ZONE_TABLE
         +------------+
       0 |            |  => NORMAL
         +------------+                                                           
       1 |            |  => DMA or NORMAL
         +------------+                                                           
       2 |            |  => HIGHMEM or NORMAL
         +------------+                                                           
       3 |            |  => BAD (DMA+HIGHMEM)
         +------------+                                                           
       4 |            |  => DMA32 or NORMAL
         +------------+                                                           
       5 |            |  => BAD (DMA+DMA32)
         +------------+                                                           
       6 |            |  => BAD (HIGHMEM+DMA32)
         +------------+                                                           
       7 |            |  => BAD (HIGHMEM+DMA32+DMA)
         +------------+                                                           
       8 |            |  => NORMAL (MOVABLE+0)
         +------------+                                                           
       9 |            |  => DMA or NORMAL (MOVABLE+DMA)
         +------------+                                                           
       a |            |  => MOVABLE (Movable is valid only if HIGHMEM is set too)
         +------------+                                                           
       b |            |  => BAD (MOVABLE+HIGHMEM+DMA)
         +------------+                                                           
       c |            |  => DMA32 or NORMAL (MOVABLE+DMA32)
         +------------+                                                           
       d |            |  => BAD (MOVABLE+DMA32+DMA)
         +------------+                                                           
       e |            |  => BAD (MOVABLE+DMA32+HIGHMEM)
         +------------+                                                           
       f |            |  => BAD (MOVABLE+DMA32+HIGHMEM+DMA)
         +------------+

