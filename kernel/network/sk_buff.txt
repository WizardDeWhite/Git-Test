1. netdev_alloc_skb()
================================================================================
static inline struct sk_buff *netdev_alloc_skb(struct net_device *dev,
					       unsigned int length)
{
	return __netdev_alloc_skb(dev, length, GFP_ATOMIC);
}

struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
				   unsigned int length, gfp_t gfp_mask)
{
	struct sk_buff *skb = NULL;
	unsigned int fragsz = SKB_DATA_ALIGN(length + NET_SKB_PAD) +
			      SKB_DATA_ALIGN(sizeof(struct skb_shared_info));

	if (fragsz <= PAGE_SIZE && !(gfp_mask & (__GFP_WAIT | GFP_DMA))) {
		void *data;

		if (sk_memalloc_socks())
			gfp_mask |= __GFP_MEMALLOC;

		data = __netdev_alloc_frag(fragsz, gfp_mask);

		if (likely(data)) {
			skb = build_skb(data, fragsz);
			if (unlikely(!skb))
				put_page(virt_to_head_page(data));
		}
	} else {
		skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask,
				  SKB_ALLOC_RX, NUMA_NO_NODE);
	}
	if (likely(skb)) {
		skb_reserve(skb, NET_SKB_PAD);
		skb->dev = dev;
	}
	return skb;
}

1.1 __netdev_alloc_frag()
================================================================================
static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
{
	struct netdev_alloc_cache *nc;
	void *data = NULL;
	int order;
	unsigned long flags;

	local_irq_save(flags);
	nc = &__get_cpu_var(netdev_alloc_cache);
	if (unlikely(!nc->frag.page)) {
refill:
		for (order = NETDEV_FRAG_PAGE_MAX_ORDER; ;) {
			gfp_t gfp = gfp_mask;

			if (order)
				gfp |= __GFP_COMP | __GFP_NOWARN;
			nc->frag.page = alloc_pages(gfp, order);
			if (likely(nc->frag.page))
				break;
			if (--order < 0)
				goto end;
		}
		nc->frag.size = PAGE_SIZE << order;
recycle:
		atomic_set(&nc->frag.page->_count, NETDEV_PAGECNT_MAX_BIAS);
		nc->pagecnt_bias = NETDEV_PAGECNT_MAX_BIAS;
		nc->frag.offset = 0;
	}

	if (nc->frag.offset + fragsz > nc->frag.size) {
		/* avoid unnecessary locked operations if possible */
		if ((atomic_read(&nc->frag.page->_count) == nc->pagecnt_bias) ||
		    atomic_sub_and_test(nc->pagecnt_bias, &nc->frag.page->_count))
			goto recycle;
		goto refill;
	}

	data = page_address(nc->frag.page) + nc->frag.offset;
	nc->frag.offset += fragsz;
	nc->pagecnt_bias--;
end:
	local_irq_restore(flags);
	return data;
}

1.2 build_skb()
================================================================================
struct sk_buff *build_skb(void *data, unsigned int frag_size)
{
	struct skb_shared_info *shinfo;
	struct sk_buff *skb;
	unsigned int size = frag_size ? : ksize(data);

	skb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);
	if (!skb)
		return NULL;

	size -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));

	memset(skb, 0, offsetof(struct sk_buff, tail));
	skb->truesize = SKB_TRUESIZE(size);
	skb->head_frag = frag_size != 0;
	atomic_set(&skb->users, 1);
	skb->head = data;
	skb->data = data;
	skb_reset_tail_pointer(skb);
	skb->end = skb->tail + size;
	skb->mac_header = (typeof(skb->mac_header))~0U;
	skb->transport_header = (typeof(skb->transport_header))~0U;

	/* make sure we initialize shinfo sequentially */
	shinfo = skb_shinfo(skb);
	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
	atomic_set(&shinfo->dataref, 1);
	kmemcheck_annotate_variable(shinfo->destructor_arg);

	return skb;
}

1.3 __alloc_skb()
================================================================================
struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
			    int flags, int node)
{
	struct kmem_cache *cache;
	struct skb_shared_info *shinfo;
	struct sk_buff *skb;
	u8 *data;
	bool pfmemalloc;

	cache = (flags & SKB_ALLOC_FCLONE)
		? skbuff_fclone_cache : skbuff_head_cache;

	if (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))
		gfp_mask |= __GFP_MEMALLOC;

	/* Get the HEAD */
	skb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);
	if (!skb)
		goto out;
	prefetchw(skb);

	/* We do our best to align skb_shared_info on a separate cache
	 * line. It usually works because kmalloc(X > SMP_CACHE_BYTES) gives
	 * aligned memory blocks, unless SLUB/SLAB debug is enabled.
	 * Both skb->head and skb_shared_info are cache line aligned.
	 */
	size = SKB_DATA_ALIGN(size);
	size += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
	data = kmalloc_reserve(size, gfp_mask, node, &pfmemalloc);
	if (!data)
		goto nodata;
	/* kmalloc(size) might give us more room than requested.
	 * Put skb_shared_info exactly at the end of allocated zone,
	 * to allow max possible filling before reallocation.
	 */
	size = SKB_WITH_OVERHEAD(ksize(data));
	prefetchw(data + size);

	/*
	 * Only clear those fields we need to clear, not those that we will
	 * actually initialise below. Hence, don't put any more fields after
	 * the tail pointer in struct sk_buff!
	 */
	memset(skb, 0, offsetof(struct sk_buff, tail));
	/* Account for allocated memory : skb + skb->head */
	skb->truesize = SKB_TRUESIZE(size);
	skb->pfmemalloc = pfmemalloc;
	atomic_set(&skb->users, 1);
	skb->head = data;
	skb->data = data;
	skb_reset_tail_pointer(skb);
	skb->end = skb->tail + size;
	skb->mac_header = (typeof(skb->mac_header))~0U;
	skb->transport_header = (typeof(skb->transport_header))~0U;

	/* make sure we initialize shinfo sequentially */
	shinfo = skb_shinfo(skb);
	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
	atomic_set(&shinfo->dataref, 1);
	kmemcheck_annotate_variable(shinfo->destructor_arg);

	if (flags & SKB_ALLOC_FCLONE) {
		struct sk_buff *child = skb + 1;
		atomic_t *fclone_ref = (atomic_t *) (child + 1);

		kmemcheck_annotate_bitfield(child, flags1);
		kmemcheck_annotate_bitfield(child, flags2);
		skb->fclone = SKB_FCLONE_ORIG;
		atomic_set(fclone_ref, 1);

		child->fclone = SKB_FCLONE_UNAVAILABLE;
		child->pfmemalloc = pfmemalloc;
	}
out:
	return skb;
nodata:
	kmem_cache_free(cache, skb);
	skb = NULL;
	goto out;
}

            head--->+-----------------+
            data--/ |                 |
            tail-/  |                 |
                    |                 |
                    | tail room       |
                    |                 |
                    |                 |
                    |                 |
            end---->+-----------------+

    Figure 1.1 new skb

1.4 skb_reserve()
================================================================================

2. skb_reserve(), reserve head room. ONLY allowed for an empty buffer
================================================================================
static inline void skb_reserve(struct sk_buff *skb, int len)
{
	skb->data += len;
	skb->tail += len;
}

            head--->+-----------------+
                    |head room        |
                    |                 |
            data- ->|- - - - - - - - -| ---
            tail- / |                 |  ^
                    |                 |
                    |                 | len
                    |                 |
                    |                 |  v
            data--->+-----------------+ -+-
            tail--/ |                 |
                    |                 |
                    | tail room       |
                    |                 |
                    |                 |
            end---->+-----------------+

	    Move the data/tail together to a new position.
	    As I know, the "empty buffer" means the data and tail
	    are at the same position.

    Figure 2.1 after skb_reserve()

3. skb_put()
================================================================================
unsigned char *skb_put(struct sk_buff *skb, unsigned int len)
{
	unsigned char *tmp = skb_tail_pointer(skb);
	SKB_LINEAR_ASSERT(skb);
	skb->tail += len;
	skb->len  += len;
	if (unlikely(skb->tail > skb->end))
		skb_over_panic(skb, len, __builtin_return_address(0));
	return tmp;
}

            head--->+-----------------+
                    |head room        |
                    |                 |
                    |                 |
            data--->+-----------------+
                    |user data        |
            tail- ->| - - - - - - - - | ---
                    |                 |  ^
                    |                 | len
                    |                 |  v
            tail--->+-----------------+ ---
                    |                 |
                    |                 |
                    | tail room       |
                    |                 |
                    |                 |
            end---->+-----------------+

	    Advance tail by "len".

    Figure 3.1 after skb_put()

4. skb_push()
================================================================================
unsigned char *skb_push(struct sk_buff *skb, unsigned int len)
{
	skb->data -= len;
	skb->len  += len;
	if (unlikely(skb->data<skb->head))
		skb_under_panic(skb, len, __builtin_return_address(0));
	return skb->data;
}

            head--->+-----------------+
                    |head room        |
                    |                 |
                    |                 |
            data--->+-----------------+ ---
                    |UDP header       |  ^
                    |                 | len
                    |                 |
                    |                 |  v
            data- ->| - - - - - - - - | ---
                    |user data        |
                    |                 |
                    |                 |
            tail--->+-----------------+
                    |                 |
                    |                 |
                    | tail room       |
                    |                 |
                    |                 |
            end---->+-----------------+

	    Decrement data by "len".

    Figure 4.1 after skb_push() push a UDP header


            head--->+-----------------+
                    |head room        |
                    |                 |
                    |                 |
            data--->+-----------------+ ---
                    |IP header        |  ^
                    |                 | len
                    |                 |  v
            data- ->| - - - - - - - - |
                    |UDP header       |
                    |                 |
                    | - - - - - - - - |
                    |user data        |
                    |                 |
                    |                 |
            tail--->+-----------------+
                    |                 |
                    |                 |
                    | tail room       |
                    |                 |
                    |                 |
            end---->+-----------------+

	    Decrement data by "len" again.
	    So between data and tail, it contains IP/UDP header
	    and user data.

    Figure 4.2 after skb_push() push a IP header

5. functions when paged data comes to play
================================================================================

5.1 skb_is_nonlinear(), (data_len != 0) means there is paged data
================================================================================
static inline bool skb_is_nonlinear(const struct sk_buff *skb)
{
	return skb->data_len;
}

5.2 skb_headlen(), return the number of bytes in linear area
================================================================================
static inline unsigned int skb_headlen(const struct sk_buff *skb)
{
	return skb->len - skb->data_len;
}

5.3 skb_fill_page_desc(), set page info to a frag and increase the nr_frags
================================================================================
static inline void skb_fill_page_desc(struct sk_buff *skb, int i,
				      struct page *page, int off, int size)
{
	__skb_fill_page_desc(skb, i, page, off, size);
	skb_shinfo(skb)->nr_frags = i + 1;
}

5.3.1 __skb_fill_page_desc()
================================================================================
static inline void __skb_fill_page_desc(struct sk_buff *skb, int i,
					struct page *page, int off, int size)
{
	skb_frag_t *frag = &skb_shinfo(skb)->frags[i];

	/*
	 * Propagate page->pfmemalloc to the skb if we can. The problem is
	 * that not all callers have unique ownership of the page. If
	 * pfmemalloc is set, we check the mapping as a mapping implies
	 * page->index is set (index and pfmemalloc share space).
	 * If it's a valid mapping, we cannot use page->pfmemalloc but we
	 * do not lose pfmemalloc information as the pages would not be
	 * allocated using __GFP_MEMALLOC.
	 */
	frag->page.p		  = page;
	frag->page_offset	  = off;
	skb_frag_size_set(frag, size);

	page = compound_head(page);
	if (page->pfmemalloc && !page->mapping)
		skb->pfmemalloc	= true;
}

5.4 skb_header_pointer(), copy skb data to a buffer
================================================================================
; to be more accurate, if the required data is in linear space, just return
; the pointer to this space

static inline void *skb_header_pointer(const struct sk_buff *skb, int offset,
				       int len, void *buffer)
{
	int hlen = skb_headlen(skb);

	if (hlen - offset >= len)
		return skb->data + offset;

	if (skb_copy_bits(skb, offset, buffer, len) < 0)
		return NULL;

	return buffer;
}

5.5 skb_copy_bits(), copy the skb data to kernel buffer
================================================================================
int skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)
{
	int start = skb_headlen(skb);
	struct sk_buff *frag_iter;
	int i, copy;

	if (offset > (int)skb->len - len)
		goto fault;

	/* Copy header. */
	if ((copy = start - offset) > 0) {
		if (copy > len)
			copy = len;
		skb_copy_from_linear_data_offset(skb, offset, to, copy);
		if ((len -= copy) == 0)
			return 0;
		offset += copy;
		to     += copy;
	}

	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
		int end;
		skb_frag_t *f = &skb_shinfo(skb)->frags[i];

		WARN_ON(start > offset + len);

		end = start + skb_frag_size(f);
		if ((copy = end - offset) > 0) {
			u8 *vaddr;

			if (copy > len)
				copy = len;

			vaddr = kmap_atomic(skb_frag_page(f));
			memcpy(to,
			       vaddr + f->page_offset + offset - start,
			       copy);
			kunmap_atomic(vaddr);

			if ((len -= copy) == 0)
				return 0;
			offset += copy;
			to     += copy;
		}
		start = end;
	}

	skb_walk_frags(skb, frag_iter) {
		int end;

		WARN_ON(start > offset + len);

		end = start + frag_iter->len;
		if ((copy = end - offset) > 0) {
			if (copy > len)
				copy = len;
			if (skb_copy_bits(frag_iter, offset - start, to, copy))
				goto fault;
			if ((len -= copy) == 0)
				return 0;
			offset += copy;
			to     += copy;
		}
		start = end;
	}

	if (!len)
		return 0;

fault:
	return -EFAULT;
}

5.6 skb_store_bits(), store bit from kenrel buffer to skb
================================================================================
int skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)
{
	int start = skb_headlen(skb);
	struct sk_buff *frag_iter;
	int i, copy;

	if (offset > (int)skb->len - len)
		goto fault;

	if ((copy = start - offset) > 0) {
		if (copy > len)
			copy = len;
		skb_copy_to_linear_data_offset(skb, offset, from, copy);
		if ((len -= copy) == 0)
			return 0;
		offset += copy;
		from += copy;
	}

	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
		int end;

		WARN_ON(start > offset + len);

		end = start + skb_frag_size(frag);
		if ((copy = end - offset) > 0) {
			u8 *vaddr;

			if (copy > len)
				copy = len;

			vaddr = kmap_atomic(skb_frag_page(frag));
			memcpy(vaddr + frag->page_offset + offset - start,
			       from, copy);
			kunmap_atomic(vaddr);

			if ((len -= copy) == 0)
				return 0;
			offset += copy;
			from += copy;
		}
		start = end;
	}

	skb_walk_frags(skb, frag_iter) {
		int end;

		WARN_ON(start > offset + len);

		end = start + frag_iter->len;
		if ((copy = end - offset) > 0) {
			if (copy > len)
				copy = len;
			if (skb_store_bits(frag_iter, offset - start,
					   from, copy))
				goto fault;
			if ((len -= copy) == 0)
				return 0;
			offset += copy;
			from += copy;
		}
		start = end;
	}
	if (!len)
		return 0;

fault:
	return -EFAULT;
}

5.7 skb_copy_datagram_iovec(), copy skb to iov
================================================================================
int skb_copy_datagram_iovec(const struct sk_buff *skb, int offset,
			    struct iovec *to, int len)
{
	int start = skb_headlen(skb);
	int i, copy = start - offset;
	struct sk_buff *frag_iter;

	trace_skb_copy_datagram_iovec(skb, len);

	/* Copy header. */
	if (copy > 0) {
		if (copy > len)
			copy = len;
		if (memcpy_toiovec(to, skb->data + offset, copy))
			goto fault;
		if ((len -= copy) == 0)
			return 0;
		offset += copy;
	}

	/* Copy paged appendix. Hmm... why does this look so complicated? */
	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
		int end;
		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];

		WARN_ON(start > offset + len);

		end = start + skb_frag_size(frag);
		if ((copy = end - offset) > 0) {
			int err;
			u8  *vaddr;
			struct page *page = skb_frag_page(frag);

			if (copy > len)
				copy = len;
			vaddr = kmap(page);
			err = memcpy_toiovec(to, vaddr + frag->page_offset +
					     offset - start, copy);
			kunmap(page);
			if (err)
				goto fault;
			if (!(len -= copy))
				return 0;
			offset += copy;
		}
		start = end;
	}

	skb_walk_frags(skb, frag_iter) {
		int end;

		WARN_ON(start > offset + len);

		end = start + frag_iter->len;
		if ((copy = end - offset) > 0) {
			if (copy > len)
				copy = len;
			if (skb_copy_datagram_iovec(frag_iter,
						    offset - start,
						    to, copy))
				goto fault;
			if ((len -= copy) == 0)
				return 0;
			offset += copy;
		}
		start = end;
	}
	if (!len)
		return 0;

fault:
	return -EFAULT;
}

5.8 skb_copy_datagram_from_iovec(), copy data in iov to skb
================================================================================
int skb_copy_datagram_from_iovec(struct sk_buff *skb, int offset,
				 const struct iovec *from, int from_offset,
				 int len)
{
	int start = skb_headlen(skb);
	int i, copy = start - offset;
	struct sk_buff *frag_iter;

	/* Copy header. */
	if (copy > 0) {
		if (copy > len)
			copy = len;
		if (memcpy_fromiovecend(skb->data + offset, from, from_offset,
					copy))
			goto fault;
		if ((len -= copy) == 0)
			return 0;
		offset += copy;
		from_offset += copy;
	}

	/* Copy paged appendix. Hmm... why does this look so complicated? */
	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
		int end;
		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];

		WARN_ON(start > offset + len);

		end = start + skb_frag_size(frag);
		if ((copy = end - offset) > 0) {
			int err;
			u8  *vaddr;
			struct page *page = skb_frag_page(frag);

			if (copy > len)
				copy = len;
			vaddr = kmap(page);
			err = memcpy_fromiovecend(vaddr + frag->page_offset +
						  offset - start,
						  from, from_offset, copy);
			kunmap(page);
			if (err)
				goto fault;

			if (!(len -= copy))
				return 0;
			offset += copy;
			from_offset += copy;
		}
		start = end;
	}

	skb_walk_frags(skb, frag_iter) {
		int end;

		WARN_ON(start > offset + len);

		end = start + frag_iter->len;
		if ((copy = end - offset) > 0) {
			if (copy > len)
				copy = len;
			if (skb_copy_datagram_from_iovec(frag_iter,
							 offset - start,
							 from,
							 from_offset,
							 copy))
				goto fault;
			if ((len -= copy) == 0)
				return 0;
			offset += copy;
			from_offset += copy;
		}
		start = end;
	}
	if (!len)
		return 0;

fault:
	return -EFAULT;
}

6. skb_dequeue(struct sk_buff_head *list)
================================================================================
6.1 spin_lock_irqsave(&list->lock, )
================================================================================
6.2 __skb_dequeue(list)
================================================================================
6.2.1 skb = skb_peek(list)
================================================================================
6.2.2 __skb_unlink(skb, list)
================================================================================
6.2.2.1 list->qlen--
================================================================================
6.2.2.2 next = skb->next
================================================================================
6.2.2.3 prev = skb->prev
================================================================================
6.2.2.4 skb->next = skb->prev = NULL
================================================================================
6.2.2.5 next->prev = prev
================================================================================
6.2.2.6 prev->next = next
================================================================================
6.2.3 return skb
================================================================================
6.3 spin_unlock_irqsave(&list->lock, )
================================================================================

7. skb_queue_head(list, newsk)
================================================================================
7.1 spin_lock_irqsave(&list->lock, )
================================================================================
7.2 __skb_queue_head(list, newsk) -> __skb_queue_after(list, (struct sk_buff*)list, newsk)
================================================================================
7.2.1 __skb_insert(newsk, list, list->next, list)
================================================================================
7.3 spin_unlock_irqsave(&list->lock, )
================================================================================

0. data structure
================================================================================
; most of this document reference this link
; http://vger.kernel.org/~davem/skb_data.html

0.1 sk_buff
================================================================================
   sk_buff
   +---------------------+--------+--------+
   |next                 |rbnode  |list    |
   |prev                 |        |        |
   |  (struct sk_buff*)  |        |        |
   |dev      |dev_scratch|        |        |
   |net_dev *|           |        |        |
   +---------+---------+-+--------+--------+
   |sk                 |ip_defrag_offset   |
   |  (struct sock*)   |   (int)           |
   +-------------------+-------------------+
   |tstamp             |skb_mstamp_ns      |
   |  (ktime_t)        |   (u64)           |
   +-------------------+-------------------+
   |cb                                     | control buffer, like arp will put neighbour_cb
   |  (char [48])                          |
   |                                       |
   +-------------------+-------------------+
   |_skb_refdst        |tcp_tsorted_anchor |
   |   (unsigned long) |   (list_head)     |
   |destructor         |                   |
   |   (void (*)())    |                   |
   +-------------------+-------------------+
   |len                                    | tail - data
   |data_len                               | come into play when there is paged data in the SKB
   |  (unsigned int)                       | non-zero means there is data in paged area
   |mac_len                                |
   |hdr_len                                |
   |  (__u16)                              |
   +-------------------+-------------------+
   |queue_mapping                          |
   |  (__u16)                              |
   +-------------------+-------------------+
   |__cloned_offset[0] |cloned             |
   |                   |nohdr              |
   |                   |fclone             |
   |                   |peeked             |
   |                   |head_frag          |
   |                   |pfmemalloc         |
   +-------------------+-------------------+
   |headers_start[0]   |                   |
   |                   |                   |
   |                   |                   |
   +-------------------+-------------------+
   |__pkt_type_offset[]|                   |
   |                   |                   |
   |                   |                   |
   +-------------------+-------+-----------+
   |__pkt_vlan_present_offset[]|           |
   |                           |           |
   |                           |           |
   +---------------------------+-----------+
   |priority                               |
   |skb_iif                                |
   |hash                                   |
   |    (__u32)                            |
   |vlan_proto                             |
   |vlan_tci                               |
   |    (__u16)                            |
   +-------------------+-------------------+
   |mark               |reserved_tailroom  |
   |    (__u32)        |    (__u32)        |
   +-------------------+-------------------+
   |inner_protocol     |inner_ipproto      |
   |    (__be16)       |    (__u8)         |
   +-------------------+-------------------+
   |inner_transport_header                 |
   |inner_network_header                   |
   |inner_mac_header                       |
   |protocol                               |
   |transport_header                       |
   |network_header                         | offset to ip header
   |mac_header                             |
   |  (__be16)                             |
   +---------------------------------------+
   |tail                                   |
   |end                                    |
   |  (sk_buff_data_t)                     |
   +---------------------------------------+
   |data                                   | -------->+------------------+
   |head                                   |   head   |linear            |
   |  (unsigned char*)                     |   data   |space             |
   +---------------------------------------+   tail   |                  |
   |truesize                               |          |                  |
   |  (unsigned int)                       |          |                  |
   +---------------------------------------+          |                  |
   |users                                  |          |                  |
   |  (refcount_t)                         |          |                  |
   +---------------------------------------+   end -> +------------------+
                                                      |skb_shared_info   |
                                                      |  nr_frags        |       non-linear area
                                                      |  frags[]         | ----->+----------------+
                                                      |                  |       |                |
                                                      +------------------+       |                |
                                                                                 +----------------+
                                                                                 |                |
                                                                                 |                |
                                                                                 +----------------+

0.2 sk_buff_head
================================================================================

   sk_buff_head
   +---------------------------------------+
   |next                                   |
   |prev                                   |
   |    (struct sk_buff*)                  |
   |qlen                                   |
   |    (__u32)                            |
   |lock                                   |
   |    (spinlock_t)                       |
   +---------------------------------------+

0.2.1 empty sk_buff_head
================================================================================

skb_queue_head / skb_dequeue

   sk_buff_head
   +------------------------+<----+
   |next                 ---|-----+
   |prev                    |
   |    (struct sk_buff*)   |
   |qlen                    |
   |    (__u32)             |
   |lock                    |
   |    (spinlock_t)        |
   +------------------------+

0.2.2 sk_buff_head list
================================================================================


  +---------------------------------------------------------------------------------------------------+
  |                                                                                                   |
  |    sk_buff_head                    sk_buff                         sk_buff                        |
  |    +------------------------+      +------------------------+      +------------------------+     |
  +--->|next                 ---|----->|next                 ---|----->|next                 ---|-----+
       |prev                    |<-----|prev                    |<-----|prev                    |
       |    (struct sk_buff*)   |      |    (struct sk_buff*)   |      |    (struct sk_buff*)   |
       |qlen                    | = 2  +------------------------+      +------------------------+
       |    (__u32)             |
       |lock                    |
       |    (spinlock_t)        |
       +------------------------+
