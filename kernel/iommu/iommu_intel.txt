1. start_kernel()
================================================================================

1.1 mm_init() -> mem_init()
================================================================================
void __init mem_init(void)
{
	pci_iommu_alloc();

	/* clear_bss() already clear the empty_zero_page */

	register_page_bootmem_info();

	/* this will put all memory onto the freelists */
	free_all_bootmem();
	after_bootmem = 1;

	/* Register memory areas for /proc/kcore */
	kclist_add(&kcore_vsyscall, (void *)VSYSCALL_ADDR,
			 PAGE_SIZE, KCORE_OTHER);

	mem_init_print_info(NULL);
}

1.1.1 pci_iommu_alloc()
================================================================================
void __init pci_iommu_alloc(void)
{
	struct iommu_table_entry *p;

	sort_iommu_table(__iommu_table, __iommu_table_end);
	check_iommu_entries(__iommu_table, __iommu_table_end);

	for (p = __iommu_table; p < __iommu_table_end; p++) {
		if (p && p->detect && p->detect() > 0) {
			p->flags |= IOMMU_DETECTED;
			if (p->early_init)
				p->early_init();
			if (p->flags & IOMMU_FINISH_IF_DETECTED)
				break;
		}
	}
}

1.1.1.1 p->detect(), detect_intel_iommu(), set x86_init.iommu.iommu_init
================================================================================
int __init detect_intel_iommu(void)
{
	int ret;
	struct dmar_res_callback validate_drhd_cb = {
		.cb[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &dmar_validate_one_drhd,
		.ignore_unhandled = true,
	};

	down_write(&dmar_global_lock);
	ret = dmar_table_detect();
	if (ret)
		ret = !dmar_walk_dmar_table((struct acpi_table_dmar *)dmar_tbl,
					    &validate_drhd_cb);
	if (ret && !no_iommu && !iommu_detected && !dmar_disabled) {
		iommu_detected = 1;
		/* Make sure ACS will be enabled */
		pci_request_acs();
	}

#ifdef CONFIG_X86
	if (ret)
		x86_init.iommu.iommu_init = intel_iommu_init;
#endif

	early_acpi_os_unmap_memory((void __iomem *)dmar_tbl, dmar_tbl_size);
	dmar_tbl = NULL;
	up_write(&dmar_global_lock);

	return ret ? 1 : -ENODEV;
}

1.2 rest_init() -> kernel_init() -> kernel_init_freeable()
================================================================================

1.2.1 do_pre_smp_initcalls() -> pci_iommu_init()
================================================================================
static int __init pci_iommu_init(void)
{
	struct iommu_table_entry *p;
	dma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);

#ifdef CONFIG_PCI
	dma_debug_add_bus(&pci_bus_type);
#endif
	x86_init.iommu.iommu_init();

	for (p = __iommu_table; p < __iommu_table_end; p++) {
		if (p && (p->flags & IOMMU_DETECTED) && p->late_init)
			p->late_init();
	}

	return 0;
}

1.2.1.1 intel_iommu_init()
================================================================================

2. intel_iommu_init()
================================================================================
int __init intel_iommu_init(void)
{
	int ret = -ENODEV;
	struct dmar_drhd_unit *drhd;
	struct intel_iommu *iommu;

	/* VT-d is required for a TXT/tboot launch, so enforce that */
	force_on = tboot_force_iommu();

	if (iommu_init_mempool()) {
		if (force_on)
			panic("tboot: Failed to initialize iommu memory\n");
		return -ENOMEM;
	}

	down_write(&dmar_global_lock);
	if (dmar_table_init()) {
		if (force_on)
			panic("tboot: Failed to initialize DMAR table\n");
		goto out_free_dmar;
	}

	if (dmar_dev_scope_init() < 0) {
		if (force_on)
			panic("tboot: Failed to initialize DMAR device scope\n");
		goto out_free_dmar;
	}

	if (no_iommu || dmar_disabled)
		goto out_free_dmar;

	if (list_empty(&dmar_rmrr_units))
		pr_info("No RMRR found\n");

	if (list_empty(&dmar_atsr_units))
		pr_info("No ATSR found\n");

	if (dmar_init_reserved_ranges()) {
		if (force_on)
			panic("tboot: Failed to reserve iommu ranges\n");
		goto out_free_reserved_range;
	}

	init_no_remapping_devices();

	ret = init_dmars();
	if (ret) {
		if (force_on)
			panic("tboot: Failed to initialize DMARs\n");
		pr_err("Initialization failed\n");
		goto out_free_reserved_range;
	}
	up_write(&dmar_global_lock);
	pr_info("Intel(R) Virtualization Technology for Directed I/O\n");

	init_timer(&unmap_timer);
#ifdef CONFIG_SWIOTLB
	swiotlb = 0;
#endif
	dma_ops = &intel_dma_ops;

	init_iommu_pm_ops();

	for_each_active_iommu(iommu, drhd)
		iommu->iommu_dev = iommu_device_create(NULL, iommu,
						       intel_iommu_groups,
						       "%s", iommu->name);

	bus_set_iommu(&pci_bus_type, &intel_iommu_ops);
	bus_register_notifier(&pci_bus_type, &device_nb);
	if (si_domain && !hw_pass_through)
		register_memory_notifier(&intel_iommu_memory_nb);

	intel_iommu_enabled = 1;

	return 0;

out_free_reserved_range:
	put_iova_domain(&reserved_iova_list);
out_free_dmar:
	intel_iommu_free_dmars();
	up_write(&dmar_global_lock);
	iommu_exit_mempool();
	return ret;
}

2.1 iommu_init_mempool(), create iova_cache, iommu_domain_cache, iommu_devinfo_cache
================================================================================
static int __init iommu_init_mempool(void)
{
	int ret;
	ret = iova_cache_get();
	if (ret)
		return ret;

	ret = iommu_domain_cache_init();
	if (ret)
		goto domain_error;

	ret = iommu_devinfo_cache_init();
	if (!ret)
		return ret;

	kmem_cache_destroy(iommu_domain_cache);
domain_error:
	iova_cache_put();

	return -ENOMEM;
}

2.2 dmar_table_init(), see intel_dmar.txt
================================================================================
; allocate struct intel_iommu for each dmar_drhd_unit

2.3 dmar_dev_scope_init()
================================================================================

2.4 dmar_init_reserved_ranges()
================================================================================
static int dmar_init_reserved_ranges(void)
{
	struct pci_dev *pdev = NULL;
	struct iova *iova;
	int i;

	init_iova_domain(&reserved_iova_list, VTD_PAGE_SIZE, IOVA_START_PFN,
			DMA_32BIT_PFN);

	lockdep_set_class(&reserved_iova_list.iova_rbtree_lock,
		&reserved_rbtree_key);

	/* IOAPIC ranges shouldn't be accessed by DMA */
	iova = reserve_iova(&reserved_iova_list, IOVA_PFN(IOAPIC_RANGE_START),
		IOVA_PFN(IOAPIC_RANGE_END));
	if (!iova) {
		pr_err("Reserve IOAPIC range failed\n");
		return -ENODEV;
	}

	/* Reserve all PCI MMIO to avoid peer-to-peer access */
	for_each_pci_dev(pdev) {
		struct resource *r;

		for (i = 0; i < PCI_NUM_RESOURCES; i++) {
			r = &pdev->resource[i];
			if (!r->flags || !(r->flags & IORESOURCE_MEM))
				continue;
			iova = reserve_iova(&reserved_iova_list,
					    IOVA_PFN(r->start),
					    IOVA_PFN(r->end));
			if (!iova) {
				pr_err("Reserve iova failed\n");
				return -ENODEV;
			}
		}
	}
	return 0;
}

2.4.1 init_iova_domain()
================================================================================
init_iova_domain(struct iova_domain *iovad, unsigned long granule,
	unsigned long start_pfn, unsigned long pfn_32bit)
{
	/*
	 * IOVA granularity will normally be equal to the smallest
	 * supported IOMMU page size; both *must* be capable of
	 * representing individual CPU pages exactly.
	 */
	BUG_ON((granule > PAGE_SIZE) || !is_power_of_2(granule));

	spin_lock_init(&iovad->iova_rbtree_lock);
	iovad->rbroot = RB_ROOT;
	iovad->cached32_node = NULL;
	iovad->granule = granule;
	iovad->start_pfn = start_pfn;
	iovad->dma_32bit_pfn = pfn_32bit;
}

2.4.2 reserve_iova()
================================================================================

2.5 init_no_remapping_devices(), set dev->archdata.iommu for no remmapping dev
================================================================================
; set dev->archdata.iommu to DUMMY_DEVICE_DOMAIN_INFO
static void __init init_no_remapping_devices(void)
{
	struct dmar_drhd_unit *drhd;
	struct device *dev;
	int i;

	for_each_drhd_unit(drhd) {
		if (!drhd->include_all) {
			for_each_active_dev_scope(drhd->devices,
						  drhd->devices_cnt, i, dev)
				break;
			/* ignore DMAR unit if no devices exist */
			if (i == drhd->devices_cnt)
				drhd->ignored = 1;
		}
	}

	for_each_active_drhd_unit(drhd) {
		if (drhd->include_all)
			continue;

		for_each_active_dev_scope(drhd->devices,
					  drhd->devices_cnt, i, dev)
			if (!dev_is_pci(dev) || !IS_GFX_DEVICE(to_pci_dev(dev)))
				break;
		if (i < drhd->devices_cnt)
			continue;

		/* This IOMMU has *only* gfx devices. Either bypass it or
		   set the gfx_mapped flag, as appropriate */
		if (dmar_map_gfx) {
			intel_iommu_gfx_mapped = 1;
		} else {
			drhd->ignored = 1;
			for_each_active_dev_scope(drhd->devices,
						  drhd->devices_cnt, i, dev)
				dev->archdata.iommu = DUMMY_DEVICE_DOMAIN_INFO;
		}
	}
}

2.6 init_dmars()
================================================================================
static int __init init_dmars(void)
{
	struct dmar_drhd_unit *drhd;
	struct dmar_rmrr_unit *rmrr;
	bool copied_tables = false;
	struct device *dev;
	struct intel_iommu *iommu;
	int i, ret;

	/*
	 * for each drhd
	 *    allocate root
	 *    initialize and program root entry to not present
	 * endfor
	 */
	for_each_drhd_unit(drhd) {
		/*
		 * lock not needed as this is only incremented in the single
		 * threaded kernel __init code path all other access are read
		 * only
		 */
		if (g_num_of_iommus < DMAR_UNITS_SUPPORTED) {
			g_num_of_iommus++;
			continue;
		}
		pr_err_once("Exceeded %d IOMMUs\n", DMAR_UNITS_SUPPORTED);
	}

	/* Preallocate enough resources for IOMMU hot-addition */
	if (g_num_of_iommus < DMAR_UNITS_SUPPORTED)
		g_num_of_iommus = DMAR_UNITS_SUPPORTED;

	g_iommus = kcalloc(g_num_of_iommus, sizeof(struct intel_iommu *),
			GFP_KERNEL);
	if (!g_iommus) {
		pr_err("Allocating global iommu array failed\n");
		ret = -ENOMEM;
		goto error;
	}

	deferred_flush = kzalloc(g_num_of_iommus *
		sizeof(struct deferred_flush_tables), GFP_KERNEL);
	if (!deferred_flush) {
		ret = -ENOMEM;
		goto free_g_iommus;
	}

	for_each_active_iommu(iommu, drhd) {
		g_iommus[iommu->seq_id] = iommu;

		intel_iommu_init_qi(iommu);

		ret = iommu_init_domains(iommu);
		if (ret)
			goto free_iommu;

		init_translation_status(iommu);

		if (translation_pre_enabled(iommu) && !is_kdump_kernel()) {
			iommu_disable_translation(iommu);
			clear_translation_pre_enabled(iommu);
			pr_warn("Translation was enabled for %s but we are not in kdump mode\n",
				iommu->name);
		}

		/*
		 * TBD:
		 * we could share the same root & context tables
		 * among all IOMMU's. Need to Split it later.
		 */
		ret = iommu_alloc_root_entry(iommu);
		if (ret)
			goto free_iommu;

		if (translation_pre_enabled(iommu)) {
			pr_info("Translation already enabled - trying to copy translation structures\n");

			ret = copy_translation_tables(iommu);
			if (ret) {
				/*
				 * We found the IOMMU with translation
				 * enabled - but failed to copy over the
				 * old root-entry table. Try to proceed
				 * by disabling translation now and
				 * allocating a clean root-entry table.
				 * This might cause DMAR faults, but
				 * probably the dump will still succeed.
				 */
				pr_err("Failed to copy translation tables from previous kernel for %s\n",
				       iommu->name);
				iommu_disable_translation(iommu);
				clear_translation_pre_enabled(iommu);
			} else {
				pr_info("Copied translation tables from previous kernel for %s\n",
					iommu->name);
				copied_tables = true;
			}
		}

		iommu_flush_write_buffer(iommu);
		iommu_set_root_entry(iommu);
		iommu->flush.flush_context(iommu, 0, 0, 0, DMA_CCMD_GLOBAL_INVL);
		iommu->flush.flush_iotlb(iommu, 0, 0, 0, DMA_TLB_GLOBAL_FLUSH);

		if (!ecap_pass_through(iommu->ecap))
			hw_pass_through = 0;
#ifdef CONFIG_INTEL_IOMMU_SVM
		if (pasid_enabled(iommu))
			intel_svm_alloc_pasid_tables(iommu);
#endif
	}

	if (iommu_pass_through)
		iommu_identity_mapping |= IDENTMAP_ALL;

#ifdef CONFIG_INTEL_IOMMU_BROKEN_GFX_WA
	iommu_identity_mapping |= IDENTMAP_GFX;
#endif

	if (iommu_identity_mapping) {
		ret = si_domain_init(hw_pass_through);
		if (ret)
			goto free_iommu;
	}

	check_tylersburg_isoch();

	/*
	 * If we copied translations from a previous kernel in the kdump
	 * case, we can not assign the devices to domains now, as that
	 * would eliminate the old mappings. So skip this part and defer
	 * the assignment to device driver initialization time.
	 */
	if (copied_tables)
		goto domains_done;

	/*
	 * If pass through is not set or not enabled, setup context entries for
	 * identity mappings for rmrr, gfx, and isa and may fall back to static
	 * identity mapping if iommu_identity_mapping is set.
	 */
	if (iommu_identity_mapping) {
		ret = iommu_prepare_static_identity_mapping(hw_pass_through);
		if (ret) {
			pr_crit("Failed to setup IOMMU pass-through\n");
			goto free_iommu;
		}
	}
	/*
	 * For each rmrr
	 *   for each dev attached to rmrr
	 *   do
	 *     locate drhd for dev, alloc domain for dev
	 *     allocate free domain
	 *     allocate page table entries for rmrr
	 *     if context not allocated for bus
	 *           allocate and init context
	 *           set present in root table for this bus
	 *     init context with domain, translation etc
	 *    endfor
	 * endfor
	 */
	pr_info("Setting RMRR:\n");
	for_each_rmrr_units(rmrr) {
		/* some BIOS lists non-exist devices in DMAR table. */
		for_each_active_dev_scope(rmrr->devices, rmrr->devices_cnt,
					  i, dev) {
			ret = iommu_prepare_rmrr_dev(rmrr, dev);
			if (ret)
				pr_err("Mapping reserved region failed\n");
		}
	}

	iommu_prepare_isa();

domains_done:

	/*
	 * for each drhd
	 *   enable fault log
	 *   global invalidate context cache
	 *   global invalidate iotlb
	 *   enable translation
	 */
	for_each_iommu(iommu, drhd) {
		if (drhd->ignored) {
			/*
			 * we always have to disable PMRs or DMA may fail on
			 * this device
			 */
			if (force_on)
				iommu_disable_protect_mem_regions(iommu);
			continue;
		}

		iommu_flush_write_buffer(iommu);

#ifdef CONFIG_INTEL_IOMMU_SVM
		if (pasid_enabled(iommu) && ecap_prs(iommu->ecap)) {
			ret = intel_svm_enable_prq(iommu);
			if (ret)
				goto free_iommu;
		}
#endif
		ret = dmar_set_interrupt(iommu);
		if (ret)
			goto free_iommu;

		if (!translation_pre_enabled(iommu))
			iommu_enable_translation(iommu);

		iommu_disable_protect_mem_regions(iommu);
	}

	return 0;

free_iommu:
	for_each_active_iommu(iommu, drhd) {
		disable_dmar_iommu(iommu);
		free_dmar_iommu(iommu);
	}
	kfree(deferred_flush);
free_g_iommus:
	kfree(g_iommus);
error:
	return ret;
}

2.6.1 intel_iommu_init_qi()
================================================================================
static void intel_iommu_init_qi(struct intel_iommu *iommu)
{
	/*
	 * Start from the sane iommu hardware state.
	 * If the queued invalidation is already initialized by us
	 * (for example, while enabling interrupt-remapping) then
	 * we got the things already rolling from a sane state.
	 */
	if (!iommu->qi) {
		/*
		 * Clear any previous faults.
		 */
		dmar_fault(-1, iommu);
		/*
		 * Disable queued invalidation if supported and already enabled
		 * before OS handover.
		 */
		dmar_disable_qi(iommu);
	}

	if (dmar_enable_qi(iommu)) {
		/*
		 * Queued Invalidate not enabled, use Register Based Invalidate
		 */
		iommu->flush.flush_context = __iommu_flush_context;
		iommu->flush.flush_iotlb = __iommu_flush_iotlb;
		pr_info("%s: Using Register based invalidation\n",
			iommu->name);
	} else {
		iommu->flush.flush_context = qi_flush_context;
		iommu->flush.flush_iotlb = qi_flush_iotlb;
		pr_info("%s: Using Queued invalidation\n", iommu->name);
	}
}

2.6.1.1 dmar_enable_qi(), disable qi
================================================================================

2.6.2 iommu_init_domains()
================================================================================
static int iommu_init_domains(struct intel_iommu *iommu)
{
	u32 ndomains, nlongs;
	size_t size;

	ndomains = cap_ndoms(iommu->cap);
	pr_debug("%s: Number of Domains supported <%d>\n",
		 iommu->name, ndomains);
	nlongs = BITS_TO_LONGS(ndomains);

	spin_lock_init(&iommu->lock);

	iommu->domain_ids = kcalloc(nlongs, sizeof(unsigned long), GFP_KERNEL);
	if (!iommu->domain_ids) {
		pr_err("%s: Allocating domain id array failed\n",
		       iommu->name);
		return -ENOMEM;
	}

	size = ((ndomains >> 8) + 1) * sizeof(struct dmar_domain **);
	iommu->domains = kzalloc(size, GFP_KERNEL);

	if (iommu->domains) {
		size = 256 * sizeof(struct dmar_domain *);
		iommu->domains[0] = kzalloc(size, GFP_KERNEL);
	}

	if (!iommu->domains || !iommu->domains[0]) {
		pr_err("%s: Allocating domain array failed\n",
		       iommu->name);
		kfree(iommu->domain_ids);
		kfree(iommu->domains);
		iommu->domain_ids = NULL;
		iommu->domains    = NULL;
		return -ENOMEM;
	}



	/*
	 * If Caching mode is set, then invalid translations are tagged
	 * with domain-id 0, hence we need to pre-allocate it. We also
	 * use domain-id 0 as a marker for non-allocated domain-id, so
	 * make sure it is not used for a real domain.
	 */
	set_bit(0, iommu->domain_ids);

	return 0;
}

2.6.3 init_translation_status()
================================================================================
static void init_translation_status(struct intel_iommu *iommu)
{
	u32 gsts;

	gsts = readl(iommu->reg + DMAR_GSTS_REG);
	if (gsts & DMA_GSTS_TES)
		iommu->flags |= VTD_FLAG_TRANS_PRE_ENABLED;
}

2.6.4 iommu_alloc_root_entry(), why alloc an entire page?
================================================================================
static int iommu_alloc_root_entry(struct intel_iommu *iommu)
{
	struct root_entry *root;
	unsigned long flags;

	root = (struct root_entry *)alloc_pgtable_page(iommu->node);
	if (!root) {
		pr_err("Allocating root entry for %s failed\n",
			iommu->name);
		return -ENOMEM;
	}

	__iommu_flush_cache(iommu, root, ROOT_SIZE);

	spin_lock_irqsave(&iommu->lock, flags);
	iommu->root_entry = root;
	spin_unlock_irqrestore(&iommu->lock, flags);

	return 0;
}

2.6.4.1 __iommu_flush_cache(), VTD_PAGE_SIZE
================================================================================

2.6.5 iommu_flush_write_buffer()
================================================================================
static void iommu_flush_write_buffer(struct intel_iommu *iommu)
{
	u32 val;
	unsigned long flag;

	if (!rwbf_quirk && !cap_rwbf(iommu->cap))
		return;

	raw_spin_lock_irqsave(&iommu->register_lock, flag);
	writel(iommu->gcmd | DMA_GCMD_WBF, iommu->reg + DMAR_GCMD_REG);

	/* Make sure hardware complete it */
	IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG,
		      readl, (!(val & DMA_GSTS_WBFS)), val);

	raw_spin_unlock_irqrestore(&iommu->register_lock, flag);
}

2.6.6 iommu_set_root_entry(), write Root Table Address in hardware
================================================================================
static void iommu_set_root_entry(struct intel_iommu *iommu)
{
	u64 addr;
	u32 sts;
	unsigned long flag;

	addr = virt_to_phys(iommu->root_entry);
	if (ecs_enabled(iommu))
		addr |= DMA_RTADDR_RTT;

	raw_spin_lock_irqsave(&iommu->register_lock, flag);
	dmar_writeq(iommu->reg + DMAR_RTADDR_REG, addr);

	writel(iommu->gcmd | DMA_GCMD_SRTP, iommu->reg + DMAR_GCMD_REG);

	/* Make sure hardware complete it */
	IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG,
		      readl, (sts & DMA_GSTS_RTPS), sts);

	raw_spin_unlock_irqrestore(&iommu->register_lock, flag);
}

2.6.7 intel_svm_alloc_pasid_tables()
================================================================================

2.6.8 iommu_prepare_static_identity_mapping()
================================================================================
static int __init iommu_prepare_static_identity_mapping(int hw)
{
	struct pci_dev *pdev = NULL;
	struct dmar_drhd_unit *drhd;
	struct intel_iommu *iommu;
	struct device *dev;
	int i;
	int ret = 0;

	for_each_pci_dev(pdev) {
		ret = dev_prepare_static_identity_mapping(&pdev->dev, hw);
		if (ret)
			return ret;
	}

	for_each_active_iommu(iommu, drhd)
		for_each_active_dev_scope(drhd->devices, drhd->devices_cnt, i, dev) {
			struct acpi_device_physical_node *pn;
			struct acpi_device *adev;

			if (dev->bus != &acpi_bus_type)
				continue;

			adev= to_acpi_device(dev);
			mutex_lock(&adev->physical_node_lock);
			list_for_each_entry(pn, &adev->physical_node_list, node) {
				ret = dev_prepare_static_identity_mapping(pn->dev, hw);
				if (ret)
					break;
			}
			mutex_unlock(&adev->physical_node_lock);
			if (ret)
				return ret;
		}

	return 0;
}

2.6.8.1 dev_prepare_static_identity_mapping(), add info to domain
================================================================================
static int __init dev_prepare_static_identity_mapping(struct device *dev, int hw)
{
	int ret;

	if (!iommu_should_identity_map(dev, 1))
		return 0;

	ret = domain_add_dev_info(si_domain, dev);
	if (!ret)
		pr_info("%s identity mapping for device %s\n",
			hw ? "Hardware" : "Software", dev_name(dev));
	else if (ret == -ENODEV)
		/* device not associated with an iommu */
		ret = 0;

	return ret;
}

2.6.9 iommu_prepare_rmrr_dev()
================================================================================

2.6.10 dmar_set_interrupt()
================================================================================

2.6.11 iommu_enable_translation(), set TE bit in GCMD register
================================================================================
static void iommu_enable_translation(struct intel_iommu *iommu)
{
	u32 sts;
	unsigned long flags;

	raw_spin_lock_irqsave(&iommu->register_lock, flags);
	iommu->gcmd |= DMA_GCMD_TE;
	writel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);

	/* Make sure hardware complete it */
	IOMMU_WAIT_OP(iommu, DMAR_GSTS_REG,
		      readl, (sts & DMA_GSTS_TES), sts);

	raw_spin_unlock_irqrestore(&iommu->register_lock, flags);
}

2.6.12 iommu_disable_protect_mem_regions()
================================================================================

2.7 dma_ops = &intel_dma_ops;
================================================================================

2.8 init_iommu_pm_ops()
================================================================================

2.9 iommu_device_create(), /sys/devices/virtual/iommu/dmarX
================================================================================
; the sysfs hierarchy is set in device_add() -> get_device_parent()
; 1. parent is NULL 
; 2. class is iommu_class
;
; There are two directories contain the iommu sysfs
; 1. /sys/devices/virtual/iommu/dmar0
; 2. /sys/class/iommu/dmar0  (a symlink to above)
;
; ------------------------------------------------------
; $ pwd
; /sys/devices/virtual/iommu/dmar0
; $ ls
; devices  intel-iommu  power  subsystem  uevent
;
; intel-iommu is created by intel_iommu_group
;
; static struct attribute_group intel_iommu_group = {
; 	.name = "intel-iommu",
; 	.attrs = intel_iommu_attrs,
; };
;
; devices is create by iommu_class's group attribute
; and those symlinks in this directory is create by iommu_device_link()
;
; static const struct attribute_group iommu_devices_attr_group = {
; 	.name = "devices",
; 	.attrs = devices_attr,
; };
;
; subsystem is create in device_add_class_symlinks()
;
; sysfs_create_link(, "subsystem");
;
; ------------------------------------------------------
; $ pwd
; /sys/class/iommu/
;
; this is created in device_add_class_symlinks()'s second
; sysfs_create_link()

struct device *iommu_device_create(struct device *parent, void *drvdata,
				   const struct attribute_group **groups,
				   const char *fmt, ...)
{
	struct device *dev;
	va_list vargs;
	int ret;

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
	if (!dev)
		return ERR_PTR(-ENOMEM);

	device_initialize(dev);

	dev->class = &iommu_class;
	dev->parent = parent;
	dev->groups = groups;
	dev_set_drvdata(dev, drvdata);

	va_start(vargs, fmt);
	ret = kobject_set_name_vargs(&dev->kobj, fmt, vargs);
	va_end(vargs);
	if (ret)
		goto error;

	ret = device_add(dev);
	if (ret)
		goto error;

	return dev;

error:
	put_device(dev);
	return ERR_PTR(ret);
}

2.10 bus_set_iommu(&pci_bus_type, &intel_iommu_ops)
================================================================================

2.11 bus_register_notifier(&pci_bus_type, &device_nb)
================================================================================

3. intel_iommu_ops
================================================================================

3.1 add_device(), intel_iommu_add_device()
================================================================================
static int intel_iommu_add_device(struct device *dev)
{
	struct intel_iommu *iommu;
	struct iommu_group *group;
	u8 bus, devfn;

	iommu = device_to_iommu(dev, &bus, &devfn);
	if (!iommu)
		return -ENODEV;

	iommu_device_link(iommu->iommu_dev, dev);

	group = iommu_group_get_for_dev(dev);

	if (IS_ERR(group))
		return PTR_ERR(group);

	iommu_group_put(group);
	return 0;
}

3.1.1 device_to_iommu(), search dmar_drhd_unit list, return the intel_iommu contains the dev
================================================================================
static struct intel_iommu *device_to_iommu(struct device *dev, u8 *bus, u8 *devfn)
{
	struct dmar_drhd_unit *drhd = NULL;
	struct intel_iommu *iommu;
	struct device *tmp;
	struct pci_dev *ptmp, *pdev = NULL;
	u16 segment = 0;
	int i;

	if (iommu_dummy(dev))
		return NULL;

	if (dev_is_pci(dev)) {
		pdev = to_pci_dev(dev);
		segment = pci_domain_nr(pdev->bus);
	} else if (has_acpi_companion(dev))
		dev = &ACPI_COMPANION(dev)->dev;

	rcu_read_lock();
	for_each_active_iommu(iommu, drhd) {
		if (pdev && segment != drhd->segment)
			continue;

		for_each_active_dev_scope(drhd->devices,
					  drhd->devices_cnt, i, tmp) {
			if (tmp == dev) {
				*bus = drhd->devices[i].bus;
				*devfn = drhd->devices[i].devfn;
				goto out;
			}

			if (!pdev || !dev_is_pci(tmp))
				continue;

			ptmp = to_pci_dev(tmp);
			if (ptmp->subordinate &&
			    ptmp->subordinate->number <= pdev->bus->number &&
			    ptmp->subordinate->busn_res.end >= pdev->bus->number)
				goto got_pdev;
		}

		if (pdev && drhd->include_all) {
		got_pdev:
			*bus = pdev->bus->number;
			*devfn = pdev->devfn;
			goto out;
		}
	}
	iommu = NULL;
 out:
	rcu_read_unlock();

	return iommu;
}

3.1.2 iommu_device_link(iommu->iommu_dev, dev), sysfs
================================================================================
; a link like this
;
; $ pwd
; /sys/devices/virtual/iommu/dmar0/devices
; $ ll
; 0000:00:02.0 -> ../../../../pci0000:00/0000:00:02.0/

int iommu_device_link(struct device *dev, struct device *link)
{
	int ret;

	if (!dev || IS_ERR(dev))
		return -ENODEV;

	ret = sysfs_add_link_to_group(&dev->kobj, "devices",
				      &link->kobj, dev_name(link));
	if (ret)
		return ret;

	ret = sysfs_create_link_nowarn(&link->kobj, &dev->kobj, "iommu");
	if (ret)
		sysfs_remove_link_from_group(&dev->kobj, "devices",
					     dev_name(link));

	return ret;
}

3.1.3 iommu_group_get_for_dev(), get iommu_group and create domain
================================================================================

3.1.4 iommu_group_put()
================================================================================
void iommu_group_put(struct iommu_group *group)
{
	if (group)
		kobject_put(group->devices_kobj);
}

3.2 device_group(), pci_device_group()
================================================================================
struct iommu_group *pci_device_group(struct device *dev)
{
	struct pci_dev *pdev = to_pci_dev(dev);
	struct group_for_pci_data data;
	struct pci_bus *bus;
	struct iommu_group *group = NULL;
	u64 devfns[4] = { 0 };

	if (WARN_ON(!dev_is_pci(dev)))
		return ERR_PTR(-EINVAL);

	/*
	 * Find the upstream DMA alias for the device.  A device must not
	 * be aliased due to topology in order to have its own IOMMU group.
	 * If we find an alias along the way that already belongs to a
	 * group, use it.
	 */
	if (pci_for_each_dma_alias(pdev, get_pci_alias_or_group, &data))
		return data.group;

	pdev = data.pdev;

	/*
	 * Continue upstream from the point of minimum IOMMU granularity
	 * due to aliases to the point where devices are protected from
	 * peer-to-peer DMA by PCI ACS.  Again, if we find an existing
	 * group, use it.
	 */
	for (bus = pdev->bus; !pci_is_root_bus(bus); bus = bus->parent) {
		if (!bus->self)
			continue;

		if (pci_acs_path_enabled(bus->self, NULL, REQ_ACS_FLAGS))
			break;

		pdev = bus->self;

		group = iommu_group_get(&pdev->dev);
		if (group)
			return group;
	}

	/*
	 * Look for existing groups on device aliases.  If we alias another
	 * device or another device aliases us, use the same group.
	 */
	group = get_pci_alias_group(pdev, (unsigned long *)devfns);
	if (group)
		return group;

	/*
	 * Look for existing groups on non-isolated functions on the same
	 * slot and aliases of those funcions, if any.  No need to clear
	 * the search bitmap, the tested devfns are still valid.
	 */
	group = get_pci_function_alias_group(pdev, (unsigned long *)devfns);
	if (group)
		return group;

	/* No shared group found, allocate new */
	group = iommu_group_alloc();
	if (IS_ERR(group))
		return NULL;

	return group;
}

3.2.1 get_pci_alias_or_group()
================================================================================
static int get_pci_alias_or_group(struct pci_dev *pdev, u16 alias, void *opaque)
{
	struct group_for_pci_data *data = opaque;

	data->pdev = pdev;
	data->group = iommu_group_get(&pdev->dev);

	return data->group != NULL;
}

3.2.2 get_pci_alias_group()
================================================================================
static struct iommu_group *get_pci_alias_group(struct pci_dev *pdev,
					       unsigned long *devfns)
{
	struct pci_dev *tmp = NULL;
	struct iommu_group *group;

	if (test_and_set_bit(pdev->devfn & 0xff, devfns))
		return NULL;

	group = iommu_group_get(&pdev->dev);
	if (group)
		return group;

	for_each_pci_dev(tmp) {
		if (tmp == pdev || tmp->bus != pdev->bus)
			continue;

		/* We alias them or they alias us */
		if (((pdev->dev_flags & PCI_DEV_FLAGS_DMA_ALIAS_DEVFN) &&
		     pdev->dma_alias_devfn == tmp->devfn) ||
		    ((tmp->dev_flags & PCI_DEV_FLAGS_DMA_ALIAS_DEVFN) &&
		     tmp->dma_alias_devfn == pdev->devfn)) {

			group = get_pci_alias_group(tmp, devfns);
			if (group) {
				pci_dev_put(tmp);
				return group;
			}

			group = get_pci_function_alias_group(tmp, devfns);
			if (group) {
				pci_dev_put(tmp);
				return group;
			}
		}
	}

	return NULL;
}

3.2.3 get_pci_function_alias_group()
================================================================================
static struct iommu_group *get_pci_function_alias_group(struct pci_dev *pdev,
							unsigned long *devfns)
{
	struct pci_dev *tmp = NULL;
	struct iommu_group *group;

	if (!pdev->multifunction || pci_acs_enabled(pdev, REQ_ACS_FLAGS))
		return NULL;

	for_each_pci_dev(tmp) {
		if (tmp == pdev || tmp->bus != pdev->bus ||
		    PCI_SLOT(tmp->devfn) != PCI_SLOT(pdev->devfn) ||
		    pci_acs_enabled(tmp, REQ_ACS_FLAGS))
			continue;

		group = get_pci_alias_group(tmp, devfns);
		if (group) {
			pci_dev_put(tmp);
			return group;
		}
	}

	return NULL;
}

3.2.4 iommu_group_alloc()
================================================================================

3.3 domain_alloc(), just for type IOMMU_DOMAIN_UNMANAGED
================================================================================
static struct iommu_domain *intel_iommu_domain_alloc(unsigned type)
{
	struct dmar_domain *dmar_domain;
	struct iommu_domain *domain;

	if (type != IOMMU_DOMAIN_UNMANAGED)
		return NULL;

	dmar_domain = alloc_domain(DOMAIN_FLAG_VIRTUAL_MACHINE);
	if (!dmar_domain) {
		pr_err("Can't allocate dmar_domain\n");
		return NULL;
	}
	if (md_domain_init(dmar_domain, DEFAULT_DOMAIN_ADDRESS_WIDTH)) {
		pr_err("Domain initialization failed\n");
		domain_exit(dmar_domain);
		return NULL;
	}
	domain_update_iommu_cap(dmar_domain);

	domain = &dmar_domain->domain;
	domain->geometry.aperture_start = 0;
	domain->geometry.aperture_end   = __DOMAIN_MAX_ADDR(dmar_domain->gaw);
	domain->geometry.force_aperture = true;

	return domain;
}

3.3.1 domain_alloc()
================================================================================
static struct dmar_domain *alloc_domain(int flags)
{
	struct dmar_domain *domain;

	domain = alloc_domain_mem();
	if (!domain)
		return NULL;

	memset(domain, 0, sizeof(*domain));
	domain->nid = -1;
	domain->flags = flags;
	INIT_LIST_HEAD(&domain->devices);

	return domain;
}

3.3.2 md_domain_init()
================================================================================
static int md_domain_init(struct dmar_domain *domain, int guest_width)
{
	int adjust_width;

	init_iova_domain(&domain->iovad, VTD_PAGE_SIZE, IOVA_START_PFN,
			DMA_32BIT_PFN);
	domain_reserve_special_ranges(domain);

	/* calculate AGAW */
	domain->gaw = guest_width;
	adjust_width = guestwidth_to_adjustwidth(guest_width);
	domain->agaw = width_to_agaw(adjust_width);

	domain->iommu_coherency = 0;
	domain->iommu_snooping = 0;
	domain->iommu_superpage = 0;
	domain->max_addr = 0;

	/* always allocate the top pgd */
	domain->pgd = (struct dma_pte *)alloc_pgtable_page(domain->nid);
	if (!domain->pgd)
		return -ENOMEM;
	domain_flush_cache(domain, domain->pgd, PAGE_SIZE);
	return 0;
}

3.3.2.1 init_iova_domain()
================================================================================
void
init_iova_domain(struct iova_domain *iovad, unsigned long granule,
	unsigned long start_pfn, unsigned long pfn_32bit)
{
	/*
	 * IOVA granularity will normally be equal to the smallest
	 * supported IOMMU page size; both *must* be capable of
	 * representing individual CPU pages exactly.
	 */
	BUG_ON((granule > PAGE_SIZE) || !is_power_of_2(granule));

	spin_lock_init(&iovad->iova_rbtree_lock);
	iovad->rbroot = RB_ROOT;
	iovad->cached32_node = NULL;
	iovad->granule = granule;
	iovad->start_pfn = start_pfn;
	iovad->dma_32bit_pfn = pfn_32bit;
}

3.3.2.2 domain_reserve_special_ranges()
================================================================================
static void domain_reserve_special_ranges(struct dmar_domain *domain)
{
	copy_reserved_iova(&reserved_iova_list, &domain->iovad);
}

3.3.2.2 guestwidth_to_adjustwidth()
================================================================================
static inline int guestwidth_to_adjustwidth(int gaw)
{
	int agaw;
	int r = (gaw - 12) % 9;

	if (r == 0)
		agaw = gaw;
	else
		agaw = gaw + 9 - r;
	if (agaw > 64)
		agaw = 64;
	return agaw;
}

3.3.3 domain_update_iommu_cap()
================================================================================
static void domain_update_iommu_cap(struct dmar_domain *domain)
{
	domain_update_iommu_coherency(domain);
	domain->iommu_snooping = domain_update_iommu_snooping(NULL);
	domain->iommu_superpage = domain_update_iommu_superpage(NULL);
}

3.4 attach_dev(), intel_iommu_attach_device
================================================================================
static int intel_iommu_attach_device(struct iommu_domain *domain,
				     struct device *dev)
{
	struct dmar_domain *dmar_domain = to_dmar_domain(domain);
	struct intel_iommu *iommu;
	int addr_width;
	u8 bus, devfn;

	if (device_is_rmrr_locked(dev)) {
		dev_warn(dev, "Device is ineligible for IOMMU domain attach due to platform RMRR requirement.  Contact your platform vendor.\n");
		return -EPERM;
	}

	/* normally dev is not mapped */
	if (unlikely(domain_context_mapped(dev))) {
		struct dmar_domain *old_domain;

		old_domain = find_domain(dev);
		if (old_domain) {
			rcu_read_lock();
			dmar_remove_one_dev_info(old_domain, dev);
			rcu_read_unlock();

			if (!domain_type_is_vm_or_si(old_domain) &&
			     list_empty(&old_domain->devices))
				domain_exit(old_domain);
		}
	}

	iommu = device_to_iommu(dev, &bus, &devfn);
	if (!iommu)
		return -ENODEV;

	/* check if this iommu agaw is sufficient for max mapped address */
	addr_width = agaw_to_width(iommu->agaw);
	if (addr_width > cap_mgaw(iommu->cap))
		addr_width = cap_mgaw(iommu->cap);

	if (dmar_domain->max_addr > (1LL << addr_width)) {
		pr_err("%s: iommu width (%d) is not "
		       "sufficient for the mapped address (%llx)\n",
		       __func__, addr_width, dmar_domain->max_addr);
		return -EFAULT;
	}
	dmar_domain->gaw = addr_width;

	/*
	 * Knock out extra levels of page tables if necessary
	 */
	while (iommu->agaw < dmar_domain->agaw) {
		struct dma_pte *pte;

		pte = dmar_domain->pgd;
		if (dma_pte_present(pte)) {
			dmar_domain->pgd = (struct dma_pte *)
				phys_to_virt(dma_pte_addr(pte));
			free_pgtable_page(pte);
		}
		dmar_domain->agaw--;
	}

	return domain_add_dev_info(dmar_domain, dev);
}

4. domain creation
================================================================================

4.1 get_domain_for_dev()
================================================================================
; calle trace
; intel_alloc_coherent()
;     __intel_map_single()

static struct dmar_domain *get_domain_for_dev(struct device *dev, int gaw)
{
	struct device_domain_info *info = NULL;
	struct dmar_domain *domain, *tmp;
	struct intel_iommu *iommu;
	u16 req_id, dma_alias;
	unsigned long flags;
	u8 bus, devfn;

	domain = find_domain(dev);
	if (domain)
		return domain;

	iommu = device_to_iommu(dev, &bus, &devfn);
	if (!iommu)
		return NULL;

	req_id = ((u16)bus << 8) | devfn;

	if (dev_is_pci(dev)) {
		struct pci_dev *pdev = to_pci_dev(dev);

		pci_for_each_dma_alias(pdev, get_last_alias, &dma_alias);

		spin_lock_irqsave(&device_domain_lock, flags);
		info = dmar_search_domain_by_dev_info(pci_domain_nr(pdev->bus),
						      PCI_BUS_NUM(dma_alias),
						      dma_alias & 0xff);
		if (info) {
			iommu = info->iommu;
			domain = info->domain;
		}
		spin_unlock_irqrestore(&device_domain_lock, flags);

		/* DMA alias already has a domain, uses it */
		if (info)
			goto found_domain;
	}

	/* Allocate and initialize new domain for the device */
	domain = alloc_domain(0);
	if (!domain)
		return NULL;
	if (domain_init(domain, iommu, gaw)) {
		domain_exit(domain);
		return NULL;
	}

	/* register PCI DMA alias device */
	if (dev_is_pci(dev) && req_id != dma_alias) {
		tmp = dmar_insert_one_dev_info(iommu, PCI_BUS_NUM(dma_alias),
					       dma_alias & 0xff, NULL, domain);

		if (!tmp || tmp != domain) {
			domain_exit(domain);
			domain = tmp;
		}

		if (!domain)
			return NULL;
	}

found_domain:
	tmp = dmar_insert_one_dev_info(iommu, bus, devfn, dev, domain);

	if (!tmp || tmp != domain) {
		domain_exit(domain);
		domain = tmp;
	}

	return domain;
}

4.1.1 find_domain(), check dev->archdata.iommu
================================================================================
; if already set, return dmar_domain

4.1.2 device_to_iommu, if not set, search the drhd->dev_scope
================================================================================

4.1.3 alloc_domain(0)
================================================================================

4.1.4 domain_init()
================================================================================

4.1.5 dmar_insert_one_dev_info()
================================================================================

4.1.5.1 domain_attach_iommu(dmar_domain, iommu), set domain in iommu->domains
================================================================================

4.2 si_domain_init()
================================================================================
; called in init_dmars()

4.2.1 alloc_domain(DOMAIN_FLAG_STATIC_IDENTITY)
================================================================================

4.3 intel_iommu_domain_alloc()
================================================================================
; calle trace
; vfio_iommu_type1_attach_group()
;     iommu_domain_allloc()
;
; in case the device is already attached to a domain, this domain will be
; un-attached, see intel_iommu_attach_device()

static struct iommu_domain *intel_iommu_domain_alloc(unsigned type)
{
	struct dmar_domain *dmar_domain;
	struct iommu_domain *domain;

	if (type != IOMMU_DOMAIN_UNMANAGED)
		return NULL;

	dmar_domain = alloc_domain(DOMAIN_FLAG_VIRTUAL_MACHINE);
	if (!dmar_domain) {
		pr_err("Can't allocate dmar_domain\n");
		return NULL;
	}
	if (md_domain_init(dmar_domain, DEFAULT_DOMAIN_ADDRESS_WIDTH)) {
		pr_err("Domain initialization failed\n");
		domain_exit(dmar_domain);
		return NULL;
	}
	domain_update_iommu_cap(dmar_domain);

	domain = &dmar_domain->domain;
	domain->geometry.aperture_start = 0;
	domain->geometry.aperture_end   = __DOMAIN_MAX_ADDR(dmar_domain->gaw);
	domain->geometry.force_aperture = true;

	return domain;
}

4.3.1 alloc_domain(DOMAIN_FLAG_VIRTUAL_MACHINE)
================================================================================

4.3.2 md_domain_init()
================================================================================

0. data structure
================================================================================
                        
0.1 DMAR in ACPI
================================================================================
                        
                          DMA Remapping Reporting Structure
                          +----------------------------------------+
                          |                                        |
                          .                                        .
                          .                                        .
                          .                                        .
                          |                                        |
                          +----------------------------------------+
                          |DMA Remapping Hardware Unit Definition  | is represented by dmar_drhd_unit
                          |                                        |
                          |            +---------------------------+
                          |            |Register Base Address      | dmar_drhd_unit->reg_base_addr
                          |            +---------------------------+
                          |                      |                 |
                          |                      |                 |
                          +----------------------------------------+
                                                 |
                                                 |
                                                 |
                                                 v
                          +----------------------------------------+
                          |                                        |
                          .                                        .
                          .                                        .
                          .                                        .
                          |                                        |
                     020h +----------------------------------------+
                          | Root Table Address     ----+           | intel_iommu->root_entry
                          +----------------------------------------+
                          |                            |           |
                          +----------------------------------------+
                                                       |
ACPI / Hardware (?)                                    |
----------------------------------------------------------------------------
Memory                                                 |
             +-----------------------------------------+
             |
             |                                                        +---------------------------------+          
             |                                             +--------->|Dev 31, Func 7     Context Entry |   ---->  IOMMU Table for each PCI device
             +-->+---------------------------------+       |          +---------------------------------+
                 |Bus #255              Root Entry |-------+          |Dev 30, Func 7     Context Entry |
                 +---------------------------------+                  +---------------------------------+
                 |Bus #254              Root Entry |                  |Dev 29, Func 7     Context Entry |
                 +---------------------------------+                  +---------------------------------+
                 |Bus #253                         |                  |                                 |
                 +---------------------------------+                  |                                 |
                 |                                 |                  |                                 |
                 |                                 |                  .                                 .
                 |                                 |                  .                                 .
                 .                                 .                  .                                 .
                 .                                 .                  |                                 |
                 .                                 .                  |                                 |
                 |                                 |                  +---------------------------------+
                 |                                 |                  |Dev 0, Func 2      Context Entry |
                 +---------------------------------+                  +---------------------------------+
                 |Bus #2                Root Entry |                  |Dev 0, Func 1      Context Entry |
                 +---------------------------------+                  +---------------------------------+
                 |Bus #1                Root Entry |                  |Dev 0, Func 0      Context Entry |
                 +---------------------------------+                  +---------------------------------+
                 |Bus #0                Root Entry |
                 +---------------------------------+

0.2 dmar_drhd_unit
================================================================================

One dmar_drhd_unit corresponds to one DMA Remapping Hardware Unit Definition

   dmar_drhd_units
   +------------------------+
   |LIST_HEAD()             |
   +------------------------+
    |
    |
    |
    |   struct dmar_drhd_unit 
    |   +------------------------------+
    +-->|list                          |
        |     (struct list_head)       |
        +------------------------------+
        |reg_base_addr                 |   Register Base Address
        |     (u64)                    |
        +------------------------------+
        |segment                       |   PCI domain
        |     (u16)                    |   copied from hardware
        +------------------------------+
        |iommu                         |
        |     (struct intel_iommu*)    |
        +------------------------------+
        |devices_cnt                   |
        |     (int)                    |
        |devices                       |
        |     (struct dmar_dev_scope *)|
        |     +------------------------+
        |     |bus                     |
        |     |devfn                   |
        |     |    (u8)                |
        |     |dev                     |  point pci_dev's acpi device
        |     |    (struct device *)   |
        +-----+------------------------+
        |                              |
        |                              |
        +------------------------------+

                        
0.3 intel_iommu, one dmar_drhd_unit one intel_iommu
================================================================================

    g_iommus(struct intel_iommu **), point to an array of DMAR_UNITS_SUPPORTED elements
        +-------------------------+      +-------------------------+
        |[seq_id]                 | ---> |                         | --->   
        |                         |      |                         |
        +-------------------------+      +-------------------------+
                    |
                    |
                    |
                    v

        struct intel_iommu
        +------------------------------+
        |segment                       |    == dmar_drhd_unit.segment
        |     (u16)                    |    so that can get the propre dmar_drhd_unit
        +------------------------------+
        |seq_id                        |    unique id, from dmar_seq_id
        |     (int)                    |
        +------------------------------+
        |root_entry                    |
        |     (struct root_entry)      |
        |     +------------------------+
        |     |lo, hi                  |
        |     |   (u64)                |
        +-----+------------------------+
        |reg                           |    io_remapped
        |     (void __iomem *)         |
        |reg_phys                      |    == dmaru->reg_base_addr
        |reg_size                      |    
        |     (u64)                    |
        +------------------------------+
        |cap                           |
        |ecap                          |
        |     (u64)                    |
        +------------------------------+
        |agaw                          |
        |msagaw                        |
        |segment                       |
        |     (u16)                    |
        +------------------------------+
        |qi                            |    queued invalidation info
        |     (struct q_inval *)       |
        |     +------------------------+
        |     |q_lock                  |
        |     |free_head               |
        |     |free_tail               |
        |     |free_cnt                |    QI_LENGTH(256)
        |     |    (int)               |       an array of qi_desc
        |     |desc                    | ---->+-------------------+
        |     |    (struct qi_desc *)  |      |                   |
        |     |    +-------------------+      +-------------------+
        |     |    |low, high          |
        |     |    |   (u64)           |
        |     |    +-------------------+       an array of desc status
        |     |desc_status             | ---->+-------------------+
        |     |    (int *)             |      |                   |
        |     |                        |      +-------------------+
        +-----+------------------------+
        |flush                         |
        |     (struct iommu_flush)     |
        |     +------------------------+
        |     |flush_context           |
        |     |flush_iotlb             |
        |     |                        |
        +-----+------------------------+
        |domain_ids                    |    bitmap of domains (total size is cap_ndoms(iommu->cap))
        |     (unsigned long *)        |
        |domains                       |    (struct dmar_domain**)
        |     (struct dmar_domain ***) |    +-------------------+     +-------------------+
        |                              | -->|    ---+           | --> |                   |
        |                              |    +-------------------+     +-------------------+
        |                              |            |
        |                              |            |      (struct dmar_domain*)
        |                              |            +----->+----------------+
        +------------------------------+                   |                |
        |                              |            256    |                |
        |                              |            entries|                |
        +------------------------------+                   |                |
                                                           +----------------+
                                                 +---------|----            |
                                                 |         +----------------+
                                                 |         |                |
        +----------------------------------------+         +----------------+
        |
        |
        v
	struct dmar_domain
        +------------------------------------+
        |nid                                 |  numa node id
        |iommu_did[DMAR_UNITS_SUPPORTED]     |
        |iommu_refcnt[DMAR_UNITS_SUPPORTED]  |
        |iommu_count                         |
        |                                    |
        +------------------------------------+
        |gaw                                 |  max guest address width (48 by default)
        |agaw                                |  adjusted guest address width based on gaw
        |max_addr                            |  maximum mapped address
        |                                    |
        +------------------------------------+
        |pgd                                 |
        |    (struct dma_pte *)              |
        |    +-------------------------------+
        |    |val(u64)                       |
        +----+-------------------------------+
        |domain                              |
        |   (struct iommu_domain)            |
        |   +--------------------------------+
        |   |type                            |
        |   |ops(iommu_ops)                  |  set to bus->iommu_ops, = intel_iommu_ops
        |   |iova_cookie                     |
        |   |geometry                        |
        |   |  (struct iommu_domain_geometry)|
        |   |  +-----------------------------+
        |   |  |aperture_start               |
        |   |  |aperture_end                 |
        |   |  |force_aperture               |
        +---+--+-----------------------------+
        |                                    |
        |                                    |
        +------------------------------------+
        |iovad                               |
        |    (struct iova_domain)            |
        |    +-------------------------------+
        |    |rbroot                         |
        |    |  (struct rb_root)             |
        |    |cached32_node                  |
        |    |  (struct rb_node *)           |
        |    |granule                        |  VTD_PAGE_SIZE(4K)
        |    |start_pfn                      |  IOVA_START_PFN(1)
        |    |dma_32bit_pfn                  |  DMA_32BIT_PFN
        |    |                               |
        +----+-------------------------------+
        |devices                             |
        |    (struct list_head) ---+         |
        +------------------------------------+
                                   |
                                   |
        struct device_domain_info  v
        +------------------------------------+
        |dev                                 |
        |      (struct device *)             |
        |bus                                 |
        |devfn                               |
        |      (u8)                          |
        +------------------------------------+
        |domain                              |
        |      (struct dmar_domain *)        |
        +------------------------------------+
        |iommu                               |
        |      (struct intel_iommu *)        |
        +------------------------------------+

0.4 dma_ops = &intel_dma_ops, driver calls dma_alloc_coherent
================================================================================

   driver
   +--------------------+
   |dma_alloc_coherent  | ---->  dma_ops->alloc()
   +--------------------+

0.5 intel_iommu_ops, set bus->iommu_ops to intel_iommu_ops
================================================================================

    intel_iommu_ops
    +----------------------------------+
    |pgsize_bitmap                     |   INTEL_IOMMU_PGSIZES(~0xFFFUL)
    |device_group                      |   pci_device_group
    +----------------------------------+
    |add_device                        |   intel_iommu_add_device
    |remove_device                     |   intel_iommu_remove_device
    |                                  |
    +----------------------------------+
    |map                               |   intel_iommu_map
    |unmap                             |   intel_iommu_unmap
    |                                  |
    +----------------------------------+
    |domain_alloc                      |   intel_iommu_domain_alloc
    |domain_free                       |   intel_iommu_domain_free
    |                                  |
    +----------------------------------+
    |attach_dev                        |   intel_iommu_attach_device
    |detach_dev                        |   intel_iommu_detach_device
    |                                  |
    +----------------------------------+
    |iova_to_phys                      |   intel_iommu_iova_to_phys
    |                                  |
    +----------------------------------+
    |                                  |
    |                                  |
    +----------------------------------+
