1. slab_sysfs_init(), /sys/kernel/slab/
================================================================================
; iterate on slab_cache and alias_list to create sysfs
static int __init slab_sysfs_init(void)
{
	struct kmem_cache *s;
	int err;

	mutex_lock(&slab_mutex);

	slab_kset = kset_create_and_add("slab", &slab_uevent_ops, kernel_kobj);
	if (!slab_kset) {
		mutex_unlock(&slab_mutex);
		printk(KERN_ERR "Cannot register slab subsystem.\n");
		return -ENOSYS;
	}

	slab_state = FULL;

	list_for_each_entry(s, &slab_caches, list) {
		err = sysfs_slab_add(s);
		if (err)
			printk(KERN_ERR "SLUB: Unable to add boot slab %s"
						" to sysfs\n", s->name);
	}

	while (alias_list) {
		struct saved_alias *al = alias_list;

		alias_list = alias_list->next;
		err = sysfs_slab_alias(al->s, al->name);
		if (err)
			printk(KERN_ERR "SLUB: Unable to add boot slab alias"
					" %s to sysfs\n", al->name);
		kfree(al);
	}

	mutex_unlock(&slab_mutex);
	resiliency_test();
	return 0;
}

1.1 sysfs_slab_add()
================================================================================
static int sysfs_slab_add(struct kmem_cache *s)
{
	int err;
	const char *name;
	int unmergeable = slab_unmergeable(s);

	if (unmergeable) {
		/*
		 * Slabcache can never be merged so we can use the name proper.
		 * This is typically the case for debug situations. In that
		 * case we can catch duplicate names easily.
		 */
		sysfs_remove_link(&slab_kset->kobj, s->name);
		name = s->name;
	} else {
		/*
		 * Create a unique name for the slab as a target
		 * for the symlinks.
		 */
		name = create_unique_id(s);
	}

	s->kobj.kset = cache_kset(s);
	err = kobject_init_and_add(&s->kobj, &slab_ktype, NULL, "%s", name);
	if (err)
		goto out_put_kobj;

	err = sysfs_create_group(&s->kobj, &slab_attr_group);
	if (err)
		goto out_del_kobj;

#ifdef CONFIG_MEMCG_KMEM
	if (is_root_cache(s)) {
		s->memcg_kset = kset_create_and_add("cgroup", NULL, &s->kobj);
		if (!s->memcg_kset) {
			err = -ENOMEM;
			goto out_del_kobj;
		}
	}
#endif

	kobject_uevent(&s->kobj, KOBJ_ADD);
	if (!unmergeable) {
		/* Setup first alias */
		sysfs_slab_alias(s, s->name);
	}
out:
	if (!unmergeable)
		kfree(name);
	return err;
out_del_kobj:
	kobject_del(&s->kobj);
out_put_kobj:
	kobject_put(&s->kobj);
	goto out;
}

1.1.1 create_unique_id(), get slab sysfs name
================================================================================
static char *create_unique_id(struct kmem_cache *s)
{
	char *name = kmalloc(ID_STR_LENGTH, GFP_KERNEL);
	char *p = name;

	BUG_ON(!name);

	*p++ = ':';
	/*
	 * First flags affecting slabcache operations. We will only
	 * get here for aliasable slabs so we do not need to support
	 * too many flags. The flags here must cover all flags that
	 * are matched during merging to guarantee that the id is
	 * unique.
	 */
	if (s->flags & SLAB_CACHE_DMA)
		*p++ = 'd';
	if (s->flags & SLAB_RECLAIM_ACCOUNT)
		*p++ = 'a';
	if (s->flags & SLAB_DEBUG_FREE)
		*p++ = 'F';
	if (!(s->flags & SLAB_NOTRACK))
		*p++ = 't';
	if (p != name + 1)
		*p++ = '-';
	p += sprintf(p, "%07d", s->size);

	BUG_ON(p > name + ID_STR_LENGTH - 1);
	return name;
}

1.1.2 sysfs_create_group(), add sysfs attribute for each kmem_cache
================================================================================
	sysfs_create_group(&s->kobj, &slab_attr_group);

static struct attribute_group slab_attr_group = {
	.attrs = slab_attrs,
};

1.2 sysfs_slab_alias()
================================================================================
static int sysfs_slab_alias(struct kmem_cache *s, const char *name)
{
	struct saved_alias *al;

	if (slab_state == FULL) {
		/*
		 * If we have a leftover link then remove it.
		 */
		sysfs_remove_link(&slab_kset->kobj, name);
		return sysfs_create_link(&slab_kset->kobj, &s->kobj, name);
	}

	al = kmalloc(sizeof(struct saved_alias), GFP_KERNEL);
	if (!al)
		return -ENOMEM;

	al->s = s;
	al->name = name;
	al->next = alias_list;
	alias_list = al;
	return 0;
}

2. kmem_cache_init()
================================================================================

2.1 create_boot_cache(kmem_cache_node)
================================================================================

2.1.1 __kmem_cache_create()
================================================================================

2.1.1.1 kmem_cache_open()
================================================================================

2.1.1.1.1 init_kmem_cache_nodes()
================================================================================

2.1.1.1.1.1 early_kmem_cache_node_alloc(), alloc a page and add to node->partial
================================================================================

2.1.1.1.2 alloc_kmem_cache_cpus()
================================================================================

2.1.1.2 sysfs_slab_add()
================================================================================

2.2 create_boot_cache(kmem_cache)
================================================================================

2.3 bootstrap(kmem_cache)
================================================================================

2.4 bootstrap(kmem_cache_node)
================================================================================

2.5 setup_kmalloc_cache_index_table()
================================================================================

2.6 create_kmalloc_caches(0)
================================================================================

2.7 init_freelist_randomization()
================================================================================

3. kmem_cache_create()
================================================================================
struct kmem_cache *
kmem_cache_create(const char *name, size_t size, size_t align,
		  unsigned long flags, void (*ctor)(void *))
{
	struct kmem_cache *s;
	const char *cache_name;
	int err;

	get_online_cpus();
	get_online_mems();
	memcg_get_cache_ids();

	mutex_lock(&slab_mutex);

	err = kmem_cache_sanity_check(name, size);
	if (err) {
		s = NULL;	/* suppress uninit var warning */
		goto out_unlock;
	}

	/*
	 * Some allocators will constraint the set of valid flags to a subset
	 * of all flags. We expect them to define CACHE_CREATE_MASK in this
	 * case, and we'll just provide them with a sanitized version of the
	 * passed flags.
	 */
	flags &= CACHE_CREATE_MASK;

	s = __kmem_cache_alias(name, size, align, flags, ctor);
	if (s)
		goto out_unlock;

	cache_name = kstrdup_const(name, GFP_KERNEL);
	if (!cache_name) {
		err = -ENOMEM;
		goto out_unlock;
	}

	s = do_kmem_cache_create(cache_name, size, size,
				 calculate_alignment(flags, align, size),
				 flags, ctor, NULL, NULL);
	if (IS_ERR(s)) {
		err = PTR_ERR(s);
		kfree_const(cache_name);
	}

out_unlock:
	mutex_unlock(&slab_mutex);

	memcg_put_cache_ids();
	put_online_mems();
	put_online_cpus();

	if (err) {
		if (flags & SLAB_PANIC)
			panic("kmem_cache_create: Failed to create slab '%s'. Error %d\n",
				name, err);
		else {
			printk(KERN_WARNING "kmem_cache_create(%s) failed with error %d",
				name, err);
			dump_stack();
		}
		return NULL;
	}
	return s;
}

3.1 kmem_cache_sanity_check()
================================================================================

3.2 __kmem_cache_alias(), share one kmem_cache if possible
================================================================================
__kmem_cache_alias(const char *name, size_t size, size_t align,
		   unsigned long flags, void (*ctor)(void *))
{
	struct kmem_cache *s, *c;

	s = find_mergeable(size, align, flags, name, ctor);
	if (s) {
		s->refcount++;

		/*
		 * Adjust the object sizes so that we clear
		 * the complete object on kzalloc.
		 */
		s->object_size = max(s->object_size, (int)size);
		s->inuse = max_t(int, s->inuse, ALIGN(size, sizeof(void *)));

		for_each_memcg_cache(c, s) {
			c->object_size = s->object_size;
			c->inuse = max_t(int, c->inuse,
					 ALIGN(size, sizeof(void *)));
		}

		if (sysfs_slab_alias(s, name)) {
			s->refcount--;
			s = NULL;
		}
	}

	return s;
}

3.3 do_kmem_cache_create(), create new kmem_cache and add to slab_caches
================================================================================
static struct kmem_cache *
do_kmem_cache_create(const char *name, size_t object_size, size_t size,
		     size_t align, unsigned long flags, void (*ctor)(void *),
		     struct mem_cgroup *memcg, struct kmem_cache *root_cache)
{
	struct kmem_cache *s;
	int err;

	err = -ENOMEM;
	s = kmem_cache_zalloc(kmem_cache, GFP_KERNEL);
	if (!s)
		goto out;

	s->name = name;
	s->object_size = object_size;
	s->size = size;
	s->align = align;
	s->ctor = ctor;

	err = init_memcg_params(s, memcg, root_cache);
	if (err)
		goto out_free_cache;

	err = __kmem_cache_create(s, flags);
	if (err)
		goto out_free_cache;

	s->refcount = 1;
	list_add(&s->list, &slab_caches);
out:
	if (err)
		return ERR_PTR(err);
	return s;

out_free_cache:
	destroy_memcg_params(s);
	kmem_cache_free(kmem_cache, s);
	goto out;
}

3.3.1 __kmem_cache_create()
================================================================================

3.3.1.1 kmem_cache_open()
================================================================================

3.3.1.2 sysfs_slab_add()
================================================================================

4. kmem_cache_alloc()
================================================================================

4.1 slab_alloc() --> slab_alloc_node(), update cpu_slab->freelist/tid
================================================================================

4.1.1 slab_pre_alloc_hook()
================================================================================

4.1.2 __slab_alloc() --> ___slab_alloc()
================================================================================

4.1.2.1 get_freelist()
================================================================================

4.1.2.2 get_freepointer()
================================================================================

4.1.2.3 next_tid()
================================================================================

4.1.2.4 new_slab_objects(), may alloc with page by new_slab()
================================================================================

4.1.2.4.1 get_partial()
================================================================================

4.1.3 slab_post_alloc_hook()
================================================================================

5. new_slab(), --> allocate_slab() get a page
================================================================================

5.1 alloc_slab_page()
================================================================================

5.1.1 alloc_pages()
================================================================================

6. kmem_cache_free()
================================================================================

6.1 cache_from_obj()
================================================================================

6.2 slab_free()
================================================================================

6.2.1 do_slab_free()
================================================================================

6.2.1.1 set_freepointer()
================================================================================

6.2.1.2 __slab_free()
================================================================================

0. init flow
================================================================================

start_kernel()
    mm_init()
        kmem_cache_init()

0. data structure
================================================================================

0.1 slab_cache
================================================================================

    slab_caches (struct list_head)
    +-----------------------+
    |                       |
    +-----------------------+
     |
     |   struct kmem_cache
     |   +----------------------+      +----------------------+
     +-->|list(struct list_head)| ---->|list(struct list_head)|
         +----------------------+      +----------------------+

0.2 kmem_cache
================================================================================



            kmem_cache                      
            +------------------------------+
            |kobj                          |
            |    (struct kobject)          |
            +------------------------------+
            |list                          |
            |    (struct list_head)        |
            +------------------------------+
            |name                          |
            |    (char *)                  |
            +------------------------------+
            |object_size                   |   = original object size
            |inuse                         |   = ALIGN(object_size, sizeof(void *))
            |size                          |   = ALIGN(inuse + padding + debug space, s->align)
            |align                         |
            |offset                        |
            |    (int)                     |
            |                              |
            |reserved                      |
            |                              |
            +------------------------------+
            |cpu_partial                   |  determine max #objects kept in the per cpu partial lists of a processor
            |    (int)                     |
            +------------------------------+
            |flags                         |
            |    (unsigned long)           |
            |allocflags                    |
            |    (gfp_t)                   |
            +------------------------------+
            |oo                            |
            |min                           |
            |max                           |
            |   (kmem_cache_order_objects) |
            |   +--------------------------+
            |   |order                     |
            |   |order_objects             |
            +---+--------------------------+
            |cpu_slab                      |  * per cpu variable pointer
            |   (struct kmem_cache_cpu*)   |          kmem_cache_cpu
            |   +--------------------------+
            |   |tid                       |  transaction id = cpu id + event id
            |   |freelist  (void **)       |          
            |   |                          |
            |   |page      (struct page *) |
            |   |partial   (struct page *) |          
            |   +--------------------------+          
            |   |stat[NR_SLUB_STAT_ITEMS]  |          
            |   |  (unsigned)              |          
            +---+--------------------------+        
            |                              |
            |                              |
            +------------------------------+
            |node[MAX_NUMNODES]            |   one node per NUMA, slab list for all objects
            |   (struct kmem_cache_node*)  |  
            +------------------------------+  
                |                             
                |                             
                |           struct kmem_cache_node                 
                +---------> +------------------------------+                                   
                            |list_lock                     |                                   
                            |    (spinlock_t)              |                                   
                            +------------------------------+                                   
                            |partial                       |                                   
                            |                              |                                   
                            |full                          |                                   
                            |    (struct list_head)        |                                   
                            +------------------------------+                                   
                            |nr_partial                    |                                   
                            |    (unsigned long)           |                                   
                            +------------------------------+                                   
                            |nr_slabs                      |   number of slab on this node
                            |total_objects                 |   number of objs
                            |    (atomit_long_t)           |                                   
                            +------------------------------+                                   


0.2.1 kmem_cache_create()
================================================================================


            kmem_cache                      
            +------------------------------+
            |kobj                          |
            |    (struct kobject)          |
            +------------------------------+
            |list                          |
            |    (struct list_head)        |
            +------------------------------+
            |name                          |
            |    (char *)                  |
            +------------------------------+
            |cpu_slab                      |  * per cpu variable pointer
            |   (struct kmem_cache_cpu*)   |
            |   +--------------------------+
            |   |stat[NR_SLUB_STAT_ITEMS]  |
            |   |  (unsigned)              |
            |   +--------------------------+
            |   |tid                       |    #cpu
            |   |freelist  (void **)       |          
            |   |                          |
            |   |page      (struct page *) |    NULL
            |   |partial   (struct page *) |    NULL
            +---+--------------------------+
            |node[MAX_NUMNODES]            |
            |   (struct kmem_cache_node*)  |
            |   +--------------------------+
            |   |nr_partial                |    0
            |   |    (unsigned long)       |
            |   |                          |
            |   |partial                   |    empty
            |   |    (struct list_head)    |
            +---+--------------------------+


0.2.2 kmem_cache_alloc()
================================================================================

; Page retrieve priority
; 1. c->freelist, c->page
; 2. c->partial
; 3. node[]->partial
; 4. page_alloc()



0.2.2.1 ___slab_alloc(), no memory on any list, allocate from page_alloc()
================================================================================
; this is what happens for the first allocation for a kmem_cache

    kmem_cache                      
    +------------------------------+
    |kobj                          |
    |    (struct kobject)          |
    +------------------------------+
    |list                          |
    |    (struct list_head)        |
    +------------------------------+
    |name                          |
    |    (char *)                  |
    +------------------------------+
    |cpu_slab                      |  * per cpu variable pointer
    |   (struct kmem_cache_cpu*)   |
    |   +--------------------------+
    |   |stat[NR_SLUB_STAT_ITEMS]  |
    |   |  (unsigned)              |
    |   +--------------------------+
    |   |tid                       |
    |   |freelist            ------|-------------+     
    |   |    (void **)             |             |
    |   |                          |             v
    |   |page                ------|--->+--------+--------+--------+--------+
    |   |   (struct page *)        |    |        |        |        |        |
    |   |                          |    +--------+--------+--------+--------+
    |   |                          |
    |   |partial                   |    NULL
    |   |   (struct page *)        |
    |   |                          |
    +---+--------------------------+
    |node[MAX_NUMNODES]            |
    |   (struct kmem_cache_node*)  |
    |   +--------------------------+
    |   |nr_partial                |    0
    |   |    (unsigned long)       |
    |   |                          |
    |   |partial                   |    empty
    |   |    (struct list_head)    |
    +---+--------------------------+


0.2.2.2 slab_alloc_node(), has memory on c->freelist
================================================================================
; has memory on percpu list, so move the freelist

    kmem_cache                      
    +------------------------------+
    |kobj                          |
    |    (struct kobject)          |
    +------------------------------+
    |list                          |
    |    (struct list_head)        |
    +------------------------------+
    |name                          |
    |    (char *)                  |
    +------------------------------+
    |cpu_slab                      |  * per cpu variable pointer
    |   (struct kmem_cache_cpu*)   |
    |   +--------------------------+
    |   |stat[NR_SLUB_STAT_ITEMS]  |
    |   |  (unsigned)              |
    |   +--------------------------+
    |   |tid                       |
    |   |freelist            ------|----------------------+     
    |   |    (void **)             |                      |
    |   |                          |                      v
    |   |page                ------|--->+--------+--------+--------+--------+
    |   |   (struct page *)        |    |        |        |        |        |
    |   |                          |    +--------+--------+--------+--------+
    |   |                          |
    |   |partial                   |    NULL
    |   |   (struct page *)        |
    |   |                          |
    +---+--------------------------+
    |node[MAX_NUMNODES]            |
    |   (struct kmem_cache_node*)  |
    |   +--------------------------+
    |   |nr_partial                |    0
    |   |    (unsigned long)       |
    |   |                          |
    |   |partial                   |    empty
    |   |    (struct list_head)    |
    +---+--------------------------+


0.2.2.3 do_slab_free(), the object is in the page cpu_slab->page
================================================================================
; has memory on percpu list, so move the freelist

    kmem_cache                      
    +------------------------------+
    |kobj                          |
    |    (struct kobject)          |
    +------------------------------+
    |list                          |
    |    (struct list_head)        |
    +------------------------------+
    |name                          |
    |    (char *)                  |
    +------------------------------+
    |cpu_slab                      |  * per cpu variable pointer
    |   (struct kmem_cache_cpu*)   |
    |   +--------------------------+
    |   |stat[NR_SLUB_STAT_ITEMS]  |
    |   |  (unsigned)              |
    |   +--------------------------+
    |   |tid                       |                              obj to free
    |   |freelist            ------|----------------------+        |
    |   |    (void **)             |                      |        |
    |   |                          |                      v        v
    |   |page                ------|--->+--------+--------+--------+--------+
    |   |   (struct page *)        |    |        |        |        |        |
    |   |                          |    +--------+--------+--------+--------+
    |   |                          |
    |   |partial                   |    NULL
    |   |   (struct page *)        |
    |   |                          |
    +---+--------------------------+


    kmem_cache                      
    +------------------------------+
    |kobj                          |
    |    (struct kobject)          |
    +------------------------------+
    |list                          |
    |    (struct list_head)        |
    +------------------------------+
    |name                          |
    |    (char *)                  |
    +------------------------------+
    |cpu_slab                      |  * per cpu variable pointer
    |   (struct kmem_cache_cpu*)   |
    |   +--------------------------+
    |   |stat[NR_SLUB_STAT_ITEMS]  |
    |   |  (unsigned)              |
    |   +--------------------------+
    |   |tid                       |                              obj
    |   |freelist            ------|-------------------------------+
    |   |    (void **)             |                               |
    |   |                          |                               v
    |   |page                ------|--->+--------+--------+--------+--------+
    |   |   (struct page *)        |    |        |        |        |freelist|
    |   |                          |    +--------+--------+--------+--------+
    |   |                          |
    |   |partial                   |    NULL
    |   |   (struct page *)        |
    |   |                          |
    +---+--------------------------+



0.3 page, the slub
================================================================================


    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+
    |freelist                      |  first free object (list head)
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of objects in Page
    |inuse                         |  number of objects used in cpu_slab
    |frozen                        |  frozen means it is in cpu_slab
    |   (unsigned )                |
    +------------------------------+


    kmem_cache_cpu
    +------------------------------+
    |freelist                      |   first free object (list head)
    |    (void *)                  |
    +------------------------------+
    |page                          |
    |    (struct page*)            |
    |    +-------------------------+
    |    |freelist                 |   NULL
    |    |objects                  |
    |    |inuse                    |
    |    |frozen                   |   1
    |    |    (unsigned)           |
    +----+-------------------------+


0.3.1 allocate_slab, just allocated from page allocator
================================================================================


    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+
    |freelist                      |  first free object
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = objects
    |   (unsigned )                |
    |frozen                        |  = 1
    |   (unsigned )                |
    +------------------------------+


0.3.2 new_slab_objects, give a page from page allocator to slub
================================================================================

    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+
    |freelist                      |  NULL (cpu_partial will take it)
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = objects
    |   (unsigned )                |
    |frozen                        |  = 1
    |   (unsigned )                |
    +------------------------------+


0.3.3 get_freelist(), prepare a page from cpu_partial or unfreeze a page
================================================================================
; if this is a valid page, page->frozen must be set to 1, otherwise it will
; trigger a bug.
;
; and this means page->freelist must NOT be NULL and is set to NULL, so that
; it could be handled to slub
;
; and very interesting, page->inuse is set to page->objects again
; I don't get this reason yet
;
; and another question is in which case the page is set like this
; I found this is the case from "redo", after we get a page from cpu_partial,
; we are reloading the page to kmem_cache_cpu

    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+  from not-NULL to 
    |freelist                      |  NULL (cpu_partial will take it)
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = objects
    |   (unsigned )                |
    |frozen                        |  = 1
    |   (unsigned )                |
    +------------------------------+


; or if the page->freelist was NULL, then page->frozen will be set to 0 after
; this call



    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+  from NULL to
    |freelist                      |  NULL (cpu_partial will take it)
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = objects
    |   (unsigned )                |
    |frozen                        |  = 0
    |   (unsigned )                |
    +------------------------------+


0.3.4 deactivate_slab()
================================================================================

0.3.4.1 no object is used, M_FREE
================================================================================

    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+ 
    |freelist                      |  point to some obj
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = objects
    |   (unsigned )                |
    |frozen                        |  = 1
    |   (unsigned )                |
    +------------------------------+



    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+ 
    |freelist                      |  
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = 0
    |   (unsigned )                |
    |frozen                        |  = 0
    |   (unsigned )                |
    +------------------------------+


0.3.4.2 no object is used, M_FREE
================================================================================

    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+ 
    |freelist                      |  point to some obj
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = objects
    |   (unsigned )                |
    |frozen                        |  = 1
    |   (unsigned )                |
    +------------------------------+



    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+ 
    |freelist                      |  
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = 0
    |   (unsigned )                |
    |frozen                        |  = 0
    |   (unsigned )                |
    +------------------------------+



0.3.5 __slab_free(), free an obj not on c->page
================================================================================
; two cases will lead to here
; 1. the process is migrated to another cpu
; 2. the page is moved to partial or full


    struct page
    +------------------------------+
    |slab_cache                    |
    |   (struct kmem_cache *)      |
    +------------------------------+ 
    |freelist                      |  point to the obj
    |   (viod *)                   |
    +------------------------------+
    |objects                       |  number of slub objects in Page
    |inuse                         |  = objects
    |   (unsigned )                |
    |frozen                        |  = 1
    |   (unsigned )                |
    +------------------------------+



