https://www.kernel.org/doc/gorman/html/understand/understand009.html


1. __get_free_pages()
================================================================================
unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
{
	struct page *page;

	/*
	 * __get_free_pages() returns a 32-bit address, which cannot represent
	 * a highmem page
	 */
	VM_BUG_ON((gfp_mask & __GFP_HIGHMEM) != 0);

	page = alloc_pages(gfp_mask, order);
	if (!page)
		return 0;
	return (unsigned long) page_address(page);
}

1.1 alloc_pages(), when CONFIG_NUMA is defined
================================================================================
static inline struct page *
alloc_pages(gfp_t gfp_mask, unsigned int order)
{
	return alloc_pages_current(gfp_mask, order);
}

1.1.1 alloc_pages_current()
================================================================================
struct page *alloc_pages_current(gfp_t gfp, unsigned order)
{
	struct mempolicy *pol = &default_policy;
	struct page *page;
	unsigned int cpuset_mems_cookie;

	if (!in_interrupt() && !(gfp & __GFP_THISNODE))
		pol = get_task_policy(current);

retry_cpuset:
	cpuset_mems_cookie = read_mems_allowed_begin();

	/*
	 * No reference counting needed for current->mempolicy
	 * nor system default_policy
	 */
	if (pol->mode == MPOL_INTERLEAVE)
		page = alloc_page_interleave(gfp, order, interleave_nodes(pol));
	else
		page = __alloc_pages_nodemask(gfp, order,
				policy_zonelist(gfp, pol, numa_node_id()),
				policy_nodemask(gfp, pol));

	if (unlikely(!page && read_mems_allowed_retry(cpuset_mems_cookie)))
		goto retry_cpuset;

	return page;
}

1.1.1.1 __alloc_pages_nodemask()
================================================================================

1.1.1.2 policy_zonelist()
================================================================================

1.1.1.3 policy_nodemask()
================================================================================

1.2 page_address()
================================================================================
static inline void *page_address(const struct page *page)
{
	return page_to_virt(page);
}

2. __alloc_pages_nodemask()
================================================================================
struct page *
__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
			struct zonelist *zonelist, nodemask_t *nodemask)
{
	struct page *page;
	unsigned int alloc_flags = ALLOC_WMARK_LOW;
	gfp_t alloc_mask = gfp_mask; /* The gfp_t that was actually used for allocation */
	struct alloc_context ac = {
		.high_zoneidx = gfp_zone(gfp_mask),
		.zonelist = zonelist,
		.nodemask = nodemask,
		.migratetype = gfpflags_to_migratetype(gfp_mask),
	};

	if (cpusets_enabled()) {
		alloc_mask |= __GFP_HARDWALL;
		alloc_flags |= ALLOC_CPUSET;
		if (!ac.nodemask)
			ac.nodemask = &cpuset_current_mems_allowed;
	}

	gfp_mask &= gfp_allowed_mask;

	lockdep_trace_alloc(gfp_mask);

	might_sleep_if(gfp_mask & __GFP_DIRECT_RECLAIM);

	if (should_fail_alloc_page(gfp_mask, order))
		return NULL;

	/*
	 * Check the zones suitable for the gfp_mask contain at least one
	 * valid zone. It's possible to have an empty zonelist as a result
	 * of __GFP_THISNODE and a memoryless node
	 */
	if (unlikely(!zonelist->_zonerefs->zone))
		return NULL;

	if (IS_ENABLED(CONFIG_CMA) && ac.migratetype == MIGRATE_MOVABLE)
		alloc_flags |= ALLOC_CMA;

	/* Dirty zone balancing only done in the fast path */
	ac.spread_dirty_pages = (gfp_mask & __GFP_WRITE);

	/*
	 * The preferred zone is used for statistics but crucially it is
	 * also used as the starting point for the zonelist iterator. It
	 * may get reset for allocations that ignore memory policies.
	 */
	ac.preferred_zoneref = first_zones_zonelist(ac.zonelist,
					ac.high_zoneidx, ac.nodemask);
	if (!ac.preferred_zoneref->zone) {
		page = NULL;
		/*
		 * This might be due to race with cpuset_current_mems_allowed
		 * update, so make sure we retry with original nodemask in the
		 * slow path.
		 */
		goto no_zone;
	}

	/* First allocation attempt */
	page = get_page_from_freelist(alloc_mask, order, alloc_flags, &ac);
	if (likely(page))
		goto out;

no_zone:
	/*
	 * Runtime PM, block IO and its error handling path can deadlock
	 * because I/O on the device might not complete.
	 */
	alloc_mask = memalloc_noio_flags(gfp_mask);
	ac.spread_dirty_pages = false;

	/*
	 * Restore the original nodemask if it was potentially replaced with
	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
	 */
	if (unlikely(ac.nodemask != nodemask))
		ac.nodemask = nodemask;

	page = __alloc_pages_slowpath(alloc_mask, order, &ac);

out:
	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
	    unlikely(memcg_kmem_charge(page, gfp_mask, order) != 0)) {
		__free_pages(page, order);
		page = NULL;
	}

	if (kmemcheck_enabled && page)
		kmemcheck_pagealloc_alloc(page, order, gfp_mask);

	trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);

	return page;
}

2.1 get_page_from_freelist()
================================================================================

2.2 __alloc_pages_slowpath()
================================================================================

3. buffered_rmqueue()
================================================================================

3.1 rmqueue_bulk()
================================================================================

3.2 __rmqueue_smallest()
================================================================================

3.2.1 rmv_page_order(), remove a page
================================================================================

3.2.2 expand(), merge back if page is bigger then expect
================================================================================

3.3 __rmqueue()
================================================================================

3.4 __mod_zone_freepage_state()
================================================================================
static inline void __mod_zone_freepage_state(struct zone *zone, int nr_pages,
					     int migratetype)
{
	__mod_zone_page_state(zone, NR_FREE_PAGES, nr_pages);
	if (is_migrate_cma(migratetype))
		__mod_zone_page_state(zone, NR_FREE_CMA_PAGES, nr_pages);
}

3.5 zone_statistics()
================================================================================
void zone_statistics(struct zone *preferred_zone, struct zone *z, gfp_t flags)
{
	if (z->zone_pgdat == preferred_zone->zone_pgdat) {
		__inc_zone_state(z, NUMA_HIT);
	} else {
		__inc_zone_state(z, NUMA_MISS);
		__inc_zone_state(preferred_zone, NUMA_FOREIGN);
	}
	if (z->node == ((flags & __GFP_OTHER_NODE) ?
			preferred_zone->node : numa_node_id()))
		__inc_zone_state(z, NUMA_LOCAL);
	else
		__inc_zone_state(z, NUMA_OTHER);
}

4. free_all_bootmem(), release free pages to the buddy allocator for 1st time
================================================================================
; called by start_kernel->mm_init->mem_init

unsigned long __init free_all_bootmem(void)
{
	unsigned long pages;

	reset_all_zones_managed_pages();

	/*
	 * We need to use NUMA_NO_NODE instead of NODE_DATA(0)->node_id
	 *  because in some case like Node0 doesn't have RAM installed
	 *  low ram will be on Node1
	 */
	pages = free_low_memory_core_early();
	totalram_pages += pages;

	return pages;
}

4.1 reset_all_zones_managed_pages() 
================================================================================
void __init reset_all_zones_managed_pages(void)
{
	struct pglist_data *pgdat;

	if (reset_managed_pages_done)
		return;

	for_each_online_pgdat(pgdat)
		reset_node_managed_pages(pgdat);

	reset_managed_pages_done = 1;
}

4.1.1 reset_node_managed_pages() 
================================================================================
void reset_node_managed_pages(pg_data_t *pgdat)
{
	struct zone *z;

	for (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)
		z->managed_pages = 0;
}

4.2 free_low_memory_core_early()
================================================================================
static unsigned long __init free_low_memory_core_early(void)
{
	unsigned long count = 0;
	phys_addr_t start, end;
	u64 i;

	memblock_clear_hotplug(0, -1);

	for_each_free_mem_range(i, NUMA_NO_NODE, &start, &end, NULL)
		count += __free_memory_core(start, end);

#ifdef CONFIG_ARCH_DISCARD_MEMBLOCK
	{
		phys_addr_t size;

		/* Free memblock.reserved array if it was allocated */
		size = get_allocated_memblock_reserved_regions_info(&start);
		if (size)
			count += __free_memory_core(start, start + size);

		/* Free memblock.memory array if it was allocated */
		size = get_allocated_memblock_memory_regions_info(&start);
		if (size)
			count += __free_memory_core(start, start + size);
	}
#endif

	return count;
}

4.2.1 for_each_free_mem_range()
================================================================================
#define for_each_free_mem_range(i, nid, p_start, p_end, p_nid)		\
	for_each_mem_range(i, &memblock.memory, &memblock.reserved,	\
			   nid, p_start, p_end, p_nid)


#define for_each_mem_range(i, type_a, type_b, nid,			\
			   p_start, p_end, p_nid)			\
	for (i = 0, __next_mem_range(&i, nid, type_a, type_b,		\
				     p_start, p_end, p_nid);		\
	     i != (u64)ULLONG_MAX;					\
	     __next_mem_range(&i, nid, type_a, type_b,			\
			      p_start, p_end, p_nid))

4.2.1.1 __next_mem_range()
================================================================================
void __init_memblock __next_mem_range(u64 *idx, int nid,
				      struct memblock_type *type_a,
				      struct memblock_type *type_b,
				      phys_addr_t *out_start,
				      phys_addr_t *out_end, int *out_nid)
{
	int idx_a = *idx & 0xffffffff;
	int idx_b = *idx >> 32;

	if (WARN_ONCE(nid == MAX_NUMNODES,
	"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
		nid = NUMA_NO_NODE;

	for (; idx_a < type_a->cnt; idx_a++) {
		struct memblock_region *m = &type_a->regions[idx_a];

		phys_addr_t m_start = m->base;
		phys_addr_t m_end = m->base + m->size;
		int	    m_nid = memblock_get_region_node(m);

		/* only memory regions are associated with nodes, check it */
		if (nid != NUMA_NO_NODE && nid != m_nid)
			continue;

		/* skip hotpluggable memory regions if needed */
		if (movable_node_is_enabled() && memblock_is_hotpluggable(m))
			continue;

		if (!type_b) {
			if (out_start)
				*out_start = m_start;
			if (out_end)
				*out_end = m_end;
			if (out_nid)
				*out_nid = m_nid;
			idx_a++;
			*idx = (u32)idx_a | (u64)idx_b << 32;
			return;
		}

		/* scan areas before each reservation */
		for (; idx_b < type_b->cnt + 1; idx_b++) {
			struct memblock_region *r;
			phys_addr_t r_start;
			phys_addr_t r_end;

			r = &type_b->regions[idx_b];
			r_start = idx_b ? r[-1].base + r[-1].size : 0;
			r_end = idx_b < type_b->cnt ?
				r->base : ULLONG_MAX;

			/*
			 * if idx_b advanced past idx_a,
			 * break out to advance idx_a
			 */
			if (r_start >= m_end)
				break;
			/* if the two regions intersect, we're done */
			if (m_start < r_end) {
				if (out_start)
					*out_start =
						max(m_start, r_start);
				if (out_end)
					*out_end = min(m_end, r_end);
				if (out_nid)
					*out_nid = m_nid;
				/*
				 * The region which ends first is
				 * advanced for the next iteration.
				 */
				if (m_end <= r_end)
					idx_a++;
				else
					idx_b++;
				*idx = (u32)idx_a | (u64)idx_b << 32;
				return;
			}
		}
	}

	/* signal end of iteration */
	*idx = ULLONG_MAX;
}

4.2.2 __free_memory_core(), free page with max order
================================================================================
static unsigned long __init __free_memory_core(phys_addr_t start,
				 phys_addr_t end)
{
	unsigned long start_pfn = PFN_UP(start);
	unsigned long end_pfn = min_t(unsigned long,
				      PFN_DOWN(end), max_low_pfn);

	if (start_pfn > end_pfn)
		return 0;

	__free_pages_memory(start_pfn, end_pfn);

	return end_pfn - start_pfn;
}

static void __init __free_pages_memory(unsigned long start, unsigned long end)
{
	int order;

	while (start < end) {
		order = min(MAX_ORDER - 1UL, __ffs(start));

		while (start + (1UL << order) > end)
			order--;

		__free_pages_bootmem(pfn_to_page(start), order);

		start += (1UL << order);
	}
}

4.2.2.1 pfn_to_page(), the flat version
================================================================================
/* memmap is virtually contiguous.  */
#define __pfn_to_page(pfn)	(vmemmap + (pfn))
#define __page_to_pfn(page)	(unsigned long)((page) - vmemmap)

4.2.2.2 __free_pages_bootmem()
================================================================================
void __init __free_pages_bootmem(struct page *page, unsigned int order)
{
	unsigned int nr_pages = 1 << order;
	struct page *p = page;
	unsigned int loop;

	prefetchw(p);
	for (loop = 0; loop < (nr_pages - 1); loop++, p++) {
		prefetchw(p + 1);
		__ClearPageReserved(p);
		set_page_count(p, 0);
	}
	__ClearPageReserved(p);
	set_page_count(p, 0);

	page_zone(page)->managed_pages += nr_pages;
	set_page_refcounted(page);
	__free_pages(page, order);
}

4.3 totalram_pages += pages
================================================================================

5 __free_pages()
================================================================================
void __free_pages(struct page *page, unsigned int order)
{
	if (put_page_testzero(page)) {
		if (order == 0)
			free_hot_cold_page(page, false);
		else
			__free_pages_ok(page, order);
	}
}

5.1 free_hot_cold_page()
================================================================================
/*
 * Free a 0-order page
 * cold == true ? free a cold page : free a hot page
 */
void free_hot_cold_page(struct page *page, bool cold)
{
	struct zone *zone = page_zone(page);
	struct per_cpu_pages *pcp;
	unsigned long flags;
	unsigned long pfn = page_to_pfn(page);
	int migratetype;

	if (!free_pcp_prepare(page))
		return;

	migratetype = get_pfnblock_migratetype(page, pfn);
	set_pcppage_migratetype(page, migratetype);
	local_irq_save(flags);
	__count_vm_event(PGFREE);

	/*
	 * We only track unmovable, reclaimable and movable on pcp lists.
	 * Free ISOLATE pages back to the allocator because they are being
	 * offlined but treat RESERVE as movable pages so we can get those
	 * areas back if necessary. Otherwise, we may have to free
	 * excessively into the page allocator
	 */
	if (migratetype >= MIGRATE_PCPTYPES) {
		if (unlikely(is_migrate_isolate(migratetype))) {
			free_one_page(zone, page, pfn, 0, migratetype);
			goto out;
		}
		migratetype = MIGRATE_MOVABLE;
	}

	pcp = &this_cpu_ptr(zone->pageset)->pcp;
	if (!cold)
		list_add(&page->lru, &pcp->lists[migratetype]);
	else
		list_add_tail(&page->lru, &pcp->lists[migratetype]);
	pcp->count++;
	if (pcp->count >= pcp->high) {
		unsigned long batch = READ_ONCE(pcp->batch);
		free_pcppages_bulk(zone, batch, pcp);
		pcp->count -= batch;
	}

out:
	local_irq_restore(flags);
}

5.2 __free_pages_ok()
================================================================================

6 __free_one_page(), freeing function for a buddy system allocator
================================================================================
static inline void __free_one_page(struct page *page,
		unsigned long pfn,
		struct zone *zone, unsigned int order,
		int migratetype)
{
	unsigned long page_idx;
	unsigned long combined_idx;
	unsigned long uninitialized_var(buddy_idx);
	struct page *buddy;
	int max_order = MAX_ORDER;

	VM_BUG_ON(!zone_is_initialized(zone));
	VM_BUG_ON_PAGE(page->flags & PAGE_FLAGS_CHECK_AT_PREP, page);

	VM_BUG_ON(migratetype == -1);
	if (is_migrate_isolate(migratetype)) {
		/*
		 * We restrict max order of merging to prevent merge
		 * between freepages on isolate pageblock and normal
		 * pageblock. Without this, pageblock isolation
		 * could cause incorrect freepage accounting.
		 */
		max_order = min(MAX_ORDER, pageblock_order + 1);
	} else {
		__mod_zone_freepage_state(zone, 1 << order, migratetype);
	}

	page_idx = pfn & ((1 << max_order) - 1);

	VM_BUG_ON_PAGE(page_idx & ((1 << order) - 1), page);
	VM_BUG_ON_PAGE(bad_range(zone, page), page);

	while (order < max_order - 1) {
		buddy_idx = __find_buddy_index(page_idx, order);
		buddy = page + (buddy_idx - page_idx);
		if (!page_is_buddy(page, buddy, order))
			break;
		/*
		 * Our buddy is free or it is CONFIG_DEBUG_PAGEALLOC guard page,
		 * merge with it and move up one order.
		 */
		if (page_is_guard(buddy)) {
			clear_page_guard(zone, buddy, order, migratetype);
		} else {
			list_del(&buddy->lru);
			zone->free_area[order].nr_free--;
			rmv_page_order(buddy);
		}
		combined_idx = buddy_idx & page_idx;
		page = page + (combined_idx - page_idx);
		page_idx = combined_idx;
		order++;
	}
	set_page_order(page, order);

	/*
	 * If this is not the largest possible page, check if the buddy
	 * of the next-highest order is free. If it is, it's possible
	 * that pages are being freed that will coalesce soon. In case,
	 * that is happening, add the free page to the tail of the list
	 * so it's less likely to be used soon and more likely to be merged
	 * as a higher order page
	 */
	if ((order < MAX_ORDER-2) && pfn_valid_within(page_to_pfn(buddy))) {
		struct page *higher_page, *higher_buddy;
		combined_idx = buddy_idx & page_idx;
		higher_page = page + (combined_idx - page_idx);
		buddy_idx = __find_buddy_index(combined_idx, order + 1);
		higher_buddy = higher_page + (buddy_idx - combined_idx);
		if (page_is_buddy(higher_page, higher_buddy, order + 1)) {
			list_add_tail(&page->lru,
				&zone->free_area[order].free_list[migratetype]);
			goto out;
		}
	}

	list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
out:
	zone->free_area[order].nr_free++;
}

6.1 __find_buddy_index()
================================================================================
; The comment is interesting
;
; When locating the "buddy", it switch on/off the (1<<O) bit in the index
; When locating the "parent", it just clear the (1<<O)) bit in the index
;
; Which leads to "parent" = "buddy" & "`buddy", just like the code
;            combined_idx = buddy_idx & page_idx;

/*
 * Locate the struct page for both the matching buddy in our
 * pair (buddy1) and the combined O(n+1) page they form (page).
 *
 * 1) Any buddy B1 will have an order O twin B2 which satisfies
 * the following equation:
 *     B2 = B1 ^ (1 << O)
 * For example, if the starting buddy (buddy2) is #8 its order
 * 1 buddy is #10:
 *     B2 = 8 ^ (1 << 1) = 8 ^ 2 = 10
 *
 * 2) Any buddy B will have an order O+1 parent P which
 * satisfies the following equation:
 *     P = B & ~(1 << O)
 *
 * Assumption: *_mem_map is contiguous at least up to MAX_ORDER
 */
static inline unsigned long
__find_buddy_index(unsigned long page_idx, unsigned int order)
{
	return page_idx ^ (1 << order);
}

6.2 page_is_buddy()
================================================================================
static inline int page_is_buddy(struct page *page, struct page *buddy,
							unsigned int order)
{
	if (!pfn_valid_within(page_to_pfn(buddy)))
		return 0;

	if (page_is_guard(buddy) && page_order(buddy) == order) {
		if (page_zone_id(page) != page_zone_id(buddy))
			return 0;

		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);

		return 1;
	}

	if (PageBuddy(buddy) && page_order(buddy) == order) {
		/*
		 * zone check is done late to avoid uselessly
		 * calculating zone/node ids for pages that could
		 * never merge.
		 */
		if (page_zone_id(page) != page_zone_id(buddy))
			return 0;

		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);

		return 1;
	}
	return 0;
}

6.3 clear_page_guard()
================================================================================

6.4 pfn_valid_within()
================================================================================

7. free_area_init_nodes()
================================================================================
void __init free_area_init_nodes(unsigned long *max_zone_pfn)
{
	unsigned long start_pfn, end_pfn;
	int i, nid;

	/* Record where the zone boundaries are */
	memset(arch_zone_lowest_possible_pfn, 0,
				sizeof(arch_zone_lowest_possible_pfn));
	memset(arch_zone_highest_possible_pfn, 0,
				sizeof(arch_zone_highest_possible_pfn));

	start_pfn = find_min_pfn_with_active_regions();

	for (i = 0; i < MAX_NR_ZONES; i++) {
		if (i == ZONE_MOVABLE)
			continue;

		end_pfn = max(max_zone_pfn[i], start_pfn);
		arch_zone_lowest_possible_pfn[i] = start_pfn;
		arch_zone_highest_possible_pfn[i] = end_pfn;

		start_pfn = end_pfn;
	}
	arch_zone_lowest_possible_pfn[ZONE_MOVABLE] = 0;
	arch_zone_highest_possible_pfn[ZONE_MOVABLE] = 0;

	/* Find the PFNs that ZONE_MOVABLE begins at in each node */
	memset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));
	find_zone_movable_pfns_for_nodes();

	/* Print out the zone ranges */
	pr_info("Zone ranges:\n");
	for (i = 0; i < MAX_NR_ZONES; i++) {
		if (i == ZONE_MOVABLE)
			continue;
		pr_info("  %-8s ", zone_names[i]);
		if (arch_zone_lowest_possible_pfn[i] ==
				arch_zone_highest_possible_pfn[i])
			pr_cont("empty\n");
		else
			pr_cont("[mem %#018Lx-%#018Lx]\n",
				(u64)arch_zone_lowest_possible_pfn[i]
					<< PAGE_SHIFT,
				((u64)arch_zone_highest_possible_pfn[i]
					<< PAGE_SHIFT) - 1);
	}

	/* Print out the PFNs ZONE_MOVABLE begins at in each node */
	pr_info("Movable zone start for each node\n");
	for (i = 0; i < MAX_NUMNODES; i++) {
		if (zone_movable_pfn[i])
			pr_info("  Node %d: %#018Lx\n", i,
			       (u64)zone_movable_pfn[i] << PAGE_SHIFT);
	}

	/* Print out the early node map */
	pr_info("Early memory node ranges\n");
	for_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid)
		pr_info("  node %3d: [mem %#018Lx-%#018Lx]\n", nid,
			(u64)start_pfn << PAGE_SHIFT,
			((u64)end_pfn << PAGE_SHIFT) - 1);

	/* Initialise every node */
	mminit_verify_pageflags_layout();
	setup_nr_node_ids();
	for_each_online_node(nid) {
		pg_data_t *pgdat = NODE_DATA(nid);
		free_area_init_node(nid, NULL,
				find_min_pfn_for_node(nid), NULL);

		/* Any memory on that node */
		if (pgdat->node_present_pages)
			node_set_state(nid, N_MEMORY);
		check_for_memory(pgdat, nid);
	}
}

7.1 find_zone_movable_pfns_for_nodes()
================================================================================
static void __init find_zone_movable_pfns_for_nodes(void)

7.1.1 early_calculate_totalpages()
================================================================================

7.2 setup_nr_node_ids(), interesting, it stores the highest instead of the #
================================================================================

7.3 free_area_init_node(nid, NULL, min_pfn, NULL), setup pg_data for nid
================================================================================

7.3.1 calculate_node_totalpages(), pgdat->node_spanned/present_pages
================================================================================

7.3.1.1 zone_spanned_pages_in_node(), zone->spanned_pages
================================================================================

7.3.1.2 zone_absent_pages_in_node(), zone->present_pages
================================================================================

7.3.2 alloc_node_mem_map(), empty when using sparse_mem
================================================================================

7.3.3 free_area_init_core()
================================================================================

7.3.3.1 calc_memmap_size() <- interesting
================================================================================

7.3.3.2 is_highmem_idx()
================================================================================

7.3.3.3 zone_pcp_init()
================================================================================

7.3.3.4 set_pageblock_order()
================================================================================

7.3.3.5 setup_usemap()
================================================================================

7.3.3.6 init_currently_empty_zone(), init zone->frea_area[].free_list[]
================================================================================

7.3.3.7 memmap_init() -> memmap_init_zone(, MIGRATE_MOVABLE), looks this is the core function
================================================================================

7.4 node_set_state(nid, N_MEMORY)
================================================================================

7.5 check_for_memory()
================================================================================

8. memmap_init_zone()
================================================================================

8.1 __init_single_page()
================================================================================

8.1.1 set_page_links()
================================================================================

8.1.1.1 set_page_zone()
================================================================================

8.1.1.2 set_page_node()
================================================================================

8.1.2 init_page_count(), set _refcount to 1
================================================================================

8.1.3 page_mapcount_reset(), set _mapcount to -1
================================================================================

8.1.4 page_cpuid_reset_last()
================================================================================

8.1.5 set_page_address(), set virtual to __va(pfn << PAGE_SHIFT)
================================================================================

8.2 set_pageblock_migratetype(page, migratetype), the initial migratetype is set here
================================================================================

9. build_all_zonelists_init(), called by start_kernel()
================================================================================

9.1 __build_all_zonelists(NULL)
================================================================================

9.1.1 build_zonelists(pgdat)
================================================================================

9.1.1.1 find_next_best_node()
================================================================================

9.1.1.2 build_zonelists_in_node_order(pgdat)
================================================================================

9.1.1.3 build_zonelists_in_zone_order(pgdat)
================================================================================

9.1.1.4 build_thisnode_zonelists()
================================================================================

9.1.2 setup_pageset() -> stop here
================================================================================

9.2 mminit_verify_zonelist()
================================================================================

9.3 cpuset_init_current_mems_allowed()
================================================================================

10. init_per_zone_wmark_min(), called from do_pre_smp_initcalls()
================================================================================

10.1 nr_free_buffer_pages()
================================================================================
10.2 setup_per_zone_wmarks() -> __setup_per_zone_wmarks()
================================================================================
10.2.1 lowmem_pages += zone_managed_pages(zone);
================================================================================
10.2.2 tmp = (u64)pages_min * zone_managed_pages(zone);
================================================================================
10.2.3 do_div(tmp, lowmem_pages);
================================================================================
10.2.4 zone->_watermark[WMARK_MIN] = tmp;
================================================================================
10.2.5 zone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;
================================================================================
10.2.6 zone->_watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;
================================================================================
10.3 refresh_zone_stat_thresholds()
================================================================================
10.4 setup_per_zone_lowmem_reserve()
================================================================================
10.5 setup_min_unmapped_ratio()
================================================================================
10.6 setup_min_slab_ratio()
================================================================================

11. alloc_contig_range(start, end, migratetype, gfp_mask)
================================================================================
11.0 cc = {.mode = MIGRATE_SYNC, alloc_contig = true}
================================================================================
11.1 start_isolate_page_range(pfn_max_align_down(start), pfn_max_align_up(end), migratetype, 0)
================================================================================
11.1.1 set_migratetype_isolate(page, migratetype, flags)
================================================================================
11.1.1.1 unmovable = has_unmovable_pages(zone, page, migratetype, isol_flags);
================================================================================
11.1.1.2 set_pageblock_migratetype(page, MIGRATE_ISOLATE)
================================================================================
11.1.1.3 move_freepages_block(zone, page, MIGRATE_ISOLATE, NULL)
================================================================================
11.1.1.4 drain_all_pages(zone)
================================================================================
11.2 __alloc_contig_migrate_range(&cc, start, end)
================================================================================
11.2.1 mtc = {}
================================================================================
11.2.2 migrate_prep()
================================================================================
11.2.3 pfn = isolate_migratepages_range(cc, pfn, end)
================================================================================
11.2.3.1 isolate_migratepages_block(cc, pfn, , ISOLATE_UNEVICTABLE)
================================================================================
11.2.4 nr_reclaimed = reclaim_clean_pages_from_list(cc->zone, cc->migratepages)
================================================================================
11.2.5 cc->nr_migratepages -= nr_reclaimed
================================================================================
11.2.6 migrate_pages(cc->migratepages, )
================================================================================
11.3 lru_add_drain_all()
================================================================================
11.4 test_pages_isolated(outer_start, end, 0)
================================================================================
11.5 isolate_freepages_range(&cc, outer_start, end)
================================================================================
11.6 undo_isolate_page_range(pfn_max_align_down(start), pfn_max_align_up(end), migratetype)
================================================================================

0. data structure
================================================================================

0.1 arch_zone_lowest/highest_possible_pfn[]
================================================================================
; dmesg: "Zone ranges:"
; an example on x86_64

   Memory

                 16M                   4G                            6G
   [   ZONE_DMA   |      ZONE_DMA32     |            ZONE_NORMAL      ]
   ^              ^                     ^                             ^
   |              |                     |                             |
lowest_pfn[0]  highest_pfn[0]       highest_pfn[1]               highest_pfn[2]
                lowest_pfn[1]        lowest_pfn[2]


0.2 node
================================================================================

   Memory

                                3G                                   6G
   [           Node0             |                Node1               ]

0.3 node/zone
================================================================================

   Memory

                 16M                   4G                            6G
   [   ZONE_DMA   |      ZONE_DMA32     |            ZONE_NORMAL      ]
                                3G
   ^                             ^                                    ^
   |<---      Node0          --->|<---          Node1             --->|


0.4 representation
================================================================================
; node_data[] allocated in alloc_node_data()

   node_data[0]                                                node_data[1]
   +-----------------------------+                             +-----------------------------+        
   |node_id                <---+ |                             |node_id                <---+ |        
   |   (int)                   | |                             |   (int)                   | |        
   +-----------------------------+                             +-----------------------------+    
   |node_zones[MAX_NR_ZONES]   | |    [ZONE_DMA]               |node_zones[MAX_NR_ZONES]   | |    [ZONE_DMA]       
   |   (struct zone)           | |    +---------------+        |   (struct zone)           | |    +---------------+
   |   +-------------------------+    |0              |        |   +-------------------------+    |empty          |
   |   |                       | |    |16M            |        |   |                       | |    |               |
   |   |zone_pgdat         ----+ |    +---------------+        |   |zone_pgdat         ----+ |    +---------------+
   |   |                         |                             |   |                         |        
   |   |                         |    [ZONE_DMA32]             |   |                         |    [ZONE_DMA32]        
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |    |16M            |        |   |                         |    |3G             |   
   |   |                         |    |3G             |        |   |                         |    |4G             |   
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |                             |   |                         |        
   |   |                         |    [ZONE_NORMAL]            |   |                         |    [ZONE_NORMAL]       
   |   |                         |    +---------------+        |   |                         |    +---------------+   
   |   |                         |    |empty          |        |   |                         |    |4G             |   
   |   |                         |    |               |        |   |                         |    |6G             |   
   +---+-------------------------+    +---------------+        +---+-------------------------+    +---------------+


0.5 alloc_pages() called by
================================================================================

    alloc_pages()

	__get_free_pages()
	alloc_pages_vma()
	alloc_hugepage_vma()
	alloc_slab_page()

0.6 per_cpu_pages
================================================================================

0.7 compound page
================================================================================

prep_compound_page
    __SetPageHead(page)
    set_compound_head(p, page)

free_tail_pages_check
    clear_compound_head()


NULL_COMPOUND_DTOR
COMPOUND_PAGE_DTOR
HUGETLB_PAGE_DTOR
TRANSHUGE_PAGE_DTOR

    page                  page                  page                  page
    +----------------+    +----------------+    +----------------+    +----------------+
    |PG_HEAD         |    |                |    |                |    |                |
    |compound_order  |    |                |    |                |    |                |
    |compound_dtor   |    |                |    |                |    |                |
    |                |    |                |    |                |    |                |
    |                |    |compound_head + |    |compound_head + |    |compound_head + |
    +----------------+    +--------------|-+    +--------------|-+    +--------------|-+
    ^                                    |                     |                     |
    |                                    |                     |                     |
    +------------------------------------+---------------------+---------------------+

0.8 zone watermark
================================================================================

  lowmem_pages = total lowmem
  zone->_watermark[min] = (current zone lowmem * pages_min) / total lowmem

  =>  sum(zone->_watermark[WMARK_MIN]) = pages_min


  tmp = (zone_managed_pages(z) * watermark_scale_factor) / 10000

  zone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;
  zone->_watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;

  The bigger watermark_scale_factor, the bigger gap between MIN, LOW and HIGH.

0.9 lowmem reserve
================================================================================

setup_per_zone_lowmem_reserve()

abbreviation:
mp[i]: managed pages of zone[i]
ra[i]: sysctl_lowmem_reserve_ratio[i]
ls[i]: lowmem_reserve[i]


        Zone[0]       Zone[1]      Zone[2]      Zone[3]
        +-------------+-------------+------------+------------+
  ls[3] |mp[3] + mp[2]|mp[3] + mp[2]|mp[3]       |     0      |
        |+ mp[1] /    |   /         |   /        |            |
        |ra[0]        |ra[1]        |ra[2]       |            |
        +-------------+-------------+------------+------------+
  ls[2] |mp[2] + mp[1]|mp[2]        |     0      |   N/A      |
        |   /         |   /         |            |            |
        |ra[0]        |ra[1]        |            |            |
        +-------------+-------------+------------+------------+
  ls[1] |mp[1]        |     0       |   N/A      |   N/A      |
        |   /         |             |            |            |
        |ra[0]        |             |            |            |
        +-------------+-------------+------------+------------+
  ls[0] |     0       |   N/A       |   N/A      |   N/A      |
        |             |             |            |            |
        |             |             |            |            |
        +-------------+-------------+------------+------------+

The zone[i].lowmem_reserve[j] means if the allocation for zone[j] fall back to
zone[i], the allocation would succeed if we have more than lowmem_reserve[j]
memory.

The is used to check the watermark in __zone_watermark_ok()

```
	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
		return false;
```

If free_pages is less then this, watermark will alarm.
