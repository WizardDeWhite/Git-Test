1. kswapd_init()
================================================================================
static int __init kswapd_init(void)
{
	int nid, ret;

	swap_setup();
	for_each_node_state(nid, N_MEMORY)
 		kswapd_run(nid);
	ret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
					"mm/vmscan:online", kswapd_cpu_online,
					NULL);
	WARN_ON(ret < 0);
	return 0;
}

1.1 kswapd_run(), for each node with memory
================================================================================
int kswapd_run(int nid)
{
	pg_data_t *pgdat = NODE_DATA(nid);
	int ret = 0;

	if (pgdat->kswapd)
		return 0;

	pgdat->kswapd = kthread_run(kswapd, pgdat, "kswapd%d", nid);
	if (IS_ERR(pgdat->kswapd)) {
		/* failure at boot is fatal */
		BUG_ON(system_state < SYSTEM_RUNNING);
		pr_err("Failed to start kswapd on node %d\n", nid);
		ret = PTR_ERR(pgdat->kswapd);
		pgdat->kswapd = NULL;
	}
	return ret;
}

1.2 kswapd_cpu_online(), set kswapd cpu affinity
================================================================================

2. kswapd(pg_data_t *pgdat), kswapd thread, the background pageout daemon for each node
================================================================================
2.0 cpumask = cpumask_of_node(pgdat->node_id)
================================================================================
2.1 set_cpus_allowed_ptr(tsk, cpumask), set cpu affinity
================================================================================
2.1 tsk->flags |= PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD;
================================================================================
2.2 set_freezable()
================================================================================
2.3 alloc_order = reclaim_order = READ_ONCE(pgdat->kswapd_order)
================================================================================
2.3 highest_zoneidx = kswapd_highest_zoneidx(pgdat, highest_zoneidx)
================================================================================
2.3 kswapd_try_to_sleep()
================================================================================
2.3.1 prepare_to_wait(pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE)
================================================================================
2.3.2 prepare_kswapd_sleep(pgdat, reclaim_order, classzone_idx), return true if kswapd is ready to sleep
================================================================================
2.3.2.1 pgdat_balanced(pgdat, reclaim_order, classzone_idx)
================================================================================
2.3.2.2 clear_pgdat_congested(pgdat)
================================================================================
2.3.3 reset_isolation_suitable(pgtad)
================================================================================
2.3.3.1 __reset_isolation_suitable(zone), clear migrate_skip and calculate zon->compact_*
================================================================================
2.3.3.1.1 __reset_isolation_pfn(zone, pfn, check_source, check_target);
================================================================================
2.3.4 wakeup_kcompactd(pgdat, alloc_order, classzone_idx)
================================================================================
2.3.4.1 wake_up_interruptible(&pgdat->kcompactd_wait)
================================================================================
2.3.5 schedule_timeout(HZ/10), sleep till timeout
================================================================================
2.3.6 finish_wait()
================================================================================
2.3.7 prepare_to_wait()
================================================================================
2.3.8 prepare_kswapd_sleep(), real sleep till woken up by others
================================================================================
2.4 balance_pgdat(pgdat, alloc_order, highest_zoneidx)
================================================================================

3. balance_pgdat(pgdat, order, highest_zoneidx)
================================================================================
3.0 sc = {.gfp_mask = GFP_KERNEL, .order = order, .may_unmap = 1}
================================================================================
3.1 __fs_reclaim_acquire(_THIS_IP_)
================================================================================
3.2 set_reclaim_active(pgdat, highest_zoneidx)
================================================================================
3.2.1 update_reclaim_active(pgdat, highest_zoneidx, true)
================================================================================
3.2.1.1 set_bit(ZONE_RECLAIM_ACTIVE, &zone->flags)
================================================================================
3.3 sc.priority = DEF_PRIORITY
================================================================================
3.4 sc.reclaim_idx = highest_zoneidx
================================================================================
3.5 age_active_anon(pgdat, &sc)
================================================================================
3.6 nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(pgdat, sc.order, sc.gfp_mask, &nr_soft_scanned)
================================================================================
3.7 sc.nr_reclaimed += nr_soft_reclaimed
================================================================================
3.8 kswapd_shrink_node(pgdat, &sc)
================================================================================
3.9 __fs_reclaim_release(_THIS_IP_), try_to_freeze(), __fs_reclaim_acquire(_THIS_IP_)
================================================================================
3.10 sc.priority--
================================================================================
3.11 clear_reclaim_active(pgdat, highest_zoneidx)
================================================================================
3.12 snapshot_refaults(NULL, pgdat)
================================================================================
3.13 __fs_reclaim_release(_THIS_IP_)
================================================================================
3.14 return sc.order
================================================================================

3. pgdat_balanced(pgdat, order, highest_zoneidx), more free_pages than WMARK_HIGH
================================================================================
3.1 mark = high_wmark_pages(zone)
================================================================================
3.2 zone_watermark_ok_safe(zone, order, mark, classzone_idx)
================================================================================
3.2.1 free_pages = zone_page_state(zone, NR_FREE_PAGES)
================================================================================
3.2.2 free_pages = zone_page_state_snapshot(zone, NR_FREE_PAGES)
================================================================================
3.2.3 __zone_watermark_ok(zone, order, mark, classzone_idx, alloc_flags, free_pages)
================================================================================
3.2.3.1 min = mark
================================================================================
3.2.3.2 free_pages <= min + z->lowmem_reserve[highest_zoneidx], return false
================================================================================
3.2.3.3 !order, return true
================================================================================
3.2.3.4 a high-order request, check at least one suitable page is free
================================================================================

4. shrink_node(pgdat, sc)
================================================================================
4.1 target_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat)
================================================================================
4.2 nr_reclaimed = sc->nr_reclaimed
================================================================================
4.3 memset(&sc->nr, 0, sizeof())
================================================================================
4.4 shrink_node_memcgs(pgdat, sc)
================================================================================
4.4.1 target_memcg = sc->target_mem_cgroup
================================================================================
4.4.2 memcg = mem_cgroup_iter(target_memcg, NULL, NULL)
================================================================================
4.4.3 lruvec = mem_cgroup_lruvec(memcg, pgdat)
================================================================================
4.4.4 shrink_lruvec(lruvec, sc)
================================================================================
4.4.4.1 get_scan_count(lruvec, sc, nr), calc scan count affected by priority
================================================================================
4.4.4.2 while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] || nr[LRU_INACTIVE_FILE]) 
================================================================================
4.4.4.3 shrink_list(lru, nr_to_scan, lruvec, sc), do reclaim
================================================================================
4.4.5 shrink_slab()
================================================================================
4.4.6 vmpressure(gfp_mask, memcg, false, scanned, reclaimed)
================================================================================
4.4.7 memcg = mem_cgroup_iter(target_memcg, memcg, NULL)
================================================================================
4.5 vmpressure(gfp, sc->target_mem_cgroup, tree, scanned, reclaimed), account vm pressure
================================================================================
4.6 reclaimable = true if sc->nr_reclaimed - nr_reclaimed
================================================================================
4.7 should_continue_reclaim()
================================================================================
4.7.1 compaction_suitable(zone, order, 0, idx)
================================================================================
4.7.2 pages_for_compaction = compact_gap(sc->order)
================================================================================
4.7.2.1 return 2UL << order;
================================================================================
4.7.3 return inactive_lru_pages > pages_for_compaction
================================================================================
4.8 return reclaimable
================================================================================

5. shrink_list(lru, nr_to_scan, lruvec, sc), do reclaim
================================================================================
5.1 shrink_active_list()
================================================================================
5.1.1 spin_lock_irq(&pgdat->lru_lock)
================================================================================
5.1.2 isolate_lru_pages(, &l_hold, &nr_scanned, sc, lru), isolate page to l_hold
================================================================================
5.1.2.1 skip page above sc.reclaim_idx
================================================================================
5.1.2.2 __isolate_lru_page(page, mode), remove page from its LRU
================================================================================
5.1.2.2.1 ClearPageLRU(page)
================================================================================
5.1.2.3 list_move(&page->lru, dst)
================================================================================
5.1.3 spin_unlock_irq(&pgdat->lru_lock)
================================================================================
5.1.4 page = lru_to_page(&l_hold)
================================================================================
5.1.5 list_del(&page->lru), take a page from l_hold
================================================================================
5.1.6 putback_lru_page(page) if !page_evictable()
================================================================================
5.1.7 list_add(&page->lru, &l_active) if page_referenced(), test accessed bit
================================================================================
5.1.8 list_add(&page->lru, &l_inactive)
================================================================================
5.1.9 spin_lock_irq(&pgdat->lru_lock)
================================================================================
5.1.10 move_pages_to_lru(lruvec, &l_active), depenends on PageActive
================================================================================
5.1.10.1 putback_lru_page(page), if !page_evictable()
================================================================================
5.1.10.1.1 lru_cache_add(page)
================================================================================
5.1.10.2 SetPageLRU(page)
================================================================================
5.1.10.3 lru = page_lru(page)
================================================================================
5.1.10.4 list_move(&page->lru, &lruvec->lists[lru])
================================================================================
5.1.10.5 if put_page_testzero()
================================================================================
5.1.10.6 del_page_from_lru_list(page, lruvec, lru)
================================================================================
5.1.11 move_pages_to_lru(lruvec, &l_inactive)
================================================================================
5.1.12 list_splice(&l_inactive, &l_active)
================================================================================
5.1.13 spin_unlock_irq(&pgdat->lru_lock)
================================================================================
5.1.14 free_unref_page_list(&l_active)
================================================================================
5.2 shrink_inactive_list(nr_to_scan, lruvec, sc, lru), return # pages reclaimed
================================================================================
5.2.1 too_many_isolated(pgdat, file, sc)
================================================================================
5.2.2 lru_add_drain()
================================================================================
5.2.3 isolate_lru_pages(nr_to_scan, lruvec, &page_list, &nr_scanned, sc, lru)
================================================================================
5.2.4 nr_reclaimed = shrink_page_list(&page_list, pgdat, sc, 0, &stat, false), core function
================================================================================
5.2.4.1 memset(stat, 0, sizeof(*stat))
================================================================================
5.2.4.2 page = lru_to_page(page_list)
================================================================================
5.2.4.3 list_del(&page->lru)
================================================================================
5.2.4.4 trylock_page(page)
================================================================================
5.2.4.5 page_check_dirty_writeback(page, &dirty, &writeback)
================================================================================
5.2.4.6 references = page_check_references(page, sc)
================================================================================
5.2.4.7 add_to_swap(), add page to swap cache
================================================================================
5.2.4.8 pageout(), write to backend file
================================================================================
5.2.4.7 __remove_mapping(mapping, page, true, ), remove from swapcache or pagecache
================================================================================
5.2.4.1 list_add(&page->lru, &ret_pages)
================================================================================
5.2.4.1 free_unref_page_list(&free_pages)
================================================================================
5.2.4.1 list_splice(&ret_pages, page_list)
================================================================================
5.2.5 move_pages_to_lru()
================================================================================
5.2.6 free_unref_page_list()
================================================================================

0. data structure
================================================================================

0.1 questions on vmscan
================================================================================

  * In kswapd_try_to_sleep(), if woken prematurely it reset kswapd_highest_zoneidx
    and reclaim_order. But why we don't reset it if woken maturely?
  * If reserve hugepage, would kswapd always wakeup?
  * in kswapd(), there are two order: alloc_order, reclaim_order. Why we use
    reclaim_order for kswapd_try_to_sleep() and use alloc_order for
    balance_pgdat()
  * we reset kswapd_order to 0 on each iteration. but if we do reclaim
    successfully on alloc_order, alloc_order == reclaim_order. we would check
    pgdat_balanced() with reclaim_order. would this be expected?
  * When we would trigger/stop kswapd
  * how swappiness works: commit fe35004fbf9e mm: avoid swapping out with swappiness==0
    memory access latency https://www.yumpu.com/en/document/read/30212700/direct-reclaim-the-linux-foundation
  * who can invoke direct reclaim
  * fix writeback in direct reclaim: https://lwn.net/Articles/396561/

0.2 pgdat
================================================================================

   pgdat
   +-------------------------------+
   |kswapd_order                   |
   |kswapd_classzone_idx           |
   |                               |
   +-------------------------------+
   |node_zones                     |
   |    (struct zone)              |
   |    +--------------------------+
   |    |compact_init_migrate_pfn  |
   |    |compact_cached_migrate_pfn|
   |    |                          |
   |    |compact_init_free_pfn     |
   |    |compact_cached_free_pfn   |
   |    |                          |
   |    |_watermark[NR_WMARK]      |  WMARK_MIN|LOW|HIGH
   |    |watermark_boost           |  init to 0 or set in boost_watermark()
   |    |                          |
   |    |lowmem_reserve[MAX_ZONES] |
   |    |                          |
   |    |vm_stat[]                 | NR_VM_ZONE_STAT_ITEMS
   |    |   (atomic_long_t)        |
   |    |                          |
   +----+--------------------------+
   |vm_stat[NR_VM_NODE_STAT_ITEMS] |
   |    (atomic_long_t)            |
   |                               |
   +-------------------------------+

0.2.1 node_page_state(pgdat, item)
================================================================================
return x = atomic_long_read(&pgdat->vm_stat[item]);

0.2.2 zone_page_state(zone, item)
================================================================================
return x = atomic_long_read(&zone->vm_stat[item]);

0.3 __setup_per_zone_wmarks()
================================================================================

0.3.1 WMARK_MIN
================================================================================
lowmem_pages = sum(zone_managed_pages)
_watermark[WMARK_MIN] = pages_min * (zone_managed_pages / lowmem_pages)

=>

sum(_watermark[WMARK_MIN]) = pages_min

The summation of all WMARK_MIN is pages_min.


0.3.2 WMARK_LOW|HIGH
================================================================================

     +-------------+
     |             |
     .             .
     |             |
     |             |
     +-------------+ WMARK_HIGH -+-
     |             |             ^
     |             |             d
     |             |             v
     +-------------+ WMARK_LOW  -+-
     |             |             ^
     |             |             d
     |             |             v
     +-------------+ WMARK_MIN  -+-
     |             |
     |             |
     |             |
     +-------------+

The distance between HIGH and LOW is the same as LOW and MIN, which is d here.

d = (MIN / 4) or (part of zone_managed_pages) 


0.4 setup_per_zone_lowmem_reserve()
================================================================================
refer to physical_mem_layout/page_alloc.txt

0.5 scan_control
================================================================================

   scan_control
   +-------------------------------+
   |nodemask                       |
   |    (nodemask_t*)              |
   |target_mem_cgroup              |
   |    (mem_cgroup *)             |
   +-------------------------------+
   |may_deactivate:2               |
   |force_deactivate:2             |
   |skipped_deactivate:2           |
   |may_writepage:1                |
   |may_unmap:1                    |
   |may_swap:1                     |
   |memcg_low_reclaim:1            |
   |memcg_low_skipped:1            |
   |hibernation_mode:1             |
   |compaction_ready:1             |
   |cache_trim_mode:1              |
   |file_is_tiny:1                 |
   |                               |
   +-------------------------------+
   |order                          |
   |priorit                        |
   |reclaim_idx                    |
   |    (s8)                       |
   |nr_scanned                     |
   |nr_reclaimed                   |
   |nr_to_reclaim                  |
   |    (unsigned long)            |
   |nr                             |
   |    +--------------------------+
   |    |dirty                     |
   |    |unqueued_dirty            |
   |    |congested                 |
   |    |writeback                 |
   |    |immediate                 |
   |    |file_taken                |
   |    |taken                     |
   |    +--------------------------+
   |reclaim_state                  |
   |    (struct reclaim_state)     |
   |    +--------------------------+
   |    |reclaimed_slab            |
   |    |    (unsigned long)       |
   |    +--------------------------+
   |                               |
   |                               |
   +-------------------------------+

0.6 lruvec
================================================================================
# recent_scanned/recent_rotated is updated during shrink_inactive_list().

Use mem_cgroup_lruvec() to retrieve lruvec:
  * pg_data_t.__lruvec
  * memcg.nodeinfo[nid].lruvec

   lruvec
   +-------------------------------+
   |lists[NR_LRU_LISTS]            |
   |    (struct list_head)         |
   |lru_lock                       |
   |    (spinlock_t)               |
   |anon_cost                      |
   |file_cost                      |
   |    (unsigned long)            |
   |nonresident_age                |
   |    (atomic_long_t)            |
   |flags                          |
   |    (unsigned long)            |
   |refaults[ANON_AND_FILE]        |
   |    (unsigned long)            |
   |pgdat                          |
   |    (struct pglist_data*)      |
   +-------------------------------+

0.7 big picture
================================================================================


                          +--------------------+
                          | __alloc_pages      |
                          +--------------------+
                             /            \
                         /                   \
                     /                          \
   +----------------------------+     +---------------------------------+      +----------------------------+       +-------------------+
   |get_page_from_freelist      |     |__alloc_pages_slowpath           |      |try_to_free_mem_cgroup_pages|       |kswapd             |
   +----------------------------+     |    __alloc_pages_direct_reclaim |      |                            |       |                   |
                   |                  |        __perform_reclaim        |      |                            |       +-------------------+
                   |                  |            try_to_free_pages    |      +----------------------------+                 |
                   |                  +---------------------------------+                     |                               |
                   |                                     |                                    |                               |
                   |                                     |                                    |                               |
                   |                                     +------------------------------------+                               |
                   |                                     |                                                                    |
                   |                                     |                                                                    |
                   v                                     v                                                                    v
          +-------------------+       +-------------------------------------+                     +-------------------------------------+
          |node_reclaim       |       |do_try_to_free_pages                 |                     |balance_pgdat                        |
          |  __node_reclaim   |       |    shrink_zones                     |                     |     mem_cgroup_soft_limit_reclaim   |
          +-------------------+       |        mem_cgroup_soft_limit_reclaim|                     |     kswapd_shrink_node              |
                   |                  |        shrink_node                  |                     |         shrink_node                 |
                   |                  |                                     |                     |                                     |
                   |                  +-------------------------------------+                     +-------------------------------------+
                   |                                     |                                               |
                   |                                     |                                               |
                   |                                     +-----------------------------------------------+
                   |                                     |                                               |
                   |                                     v                                               v
                   |                           +---------+---------+                 +-------------------+----------+
                   |                           |shrink_node        |                 |mem_cgroup_soft_limit_reclaim |
                   |                           |                   |                 |    mem_cgroup_soft_reclaim   |
                   |                           |                   |                 |        mem_cgroup_shrink_node|
                   |                           +-------------------+                 +------------------------------+
                   |                                    |                                     |
                   |                                    |                                     |
                   +------------------------------------+                                     |
                                                        |                                     |
                                                        v                                     |
                                               +------------------------+                     |
                                               |shrink_node             |                     |
                                               |     shrink_node_memcgs |                     |
                                               |         shrink_lruvec  |                     |
                                               |         shrink_slab    |                     |
                                               +------------------------+                     |
                                                           |                                  |
                                                           +-----------------+----------------+
                                                                             |
                                                                             v
                                                              +---------------------------------+
                                                              |   shrink_lruvec                 |
                                                              |       shrink_list               |
                                                              |           shrink_inactive_list  |
                                                              +---------------------------------+
                                                                             |
                                                                             v
                                                                 +-----------------------+
		                                                 |shrink_page_list       |
                                                                 |                       |
                                                                 +-----------------------+
